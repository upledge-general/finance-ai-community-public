{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM3GPM/QHvBLPHhgMI1kW1/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/upledge-general/finance-ai-community-public/blob/main/data_summarizer_insights.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå Demo Notebook ‚Äî Please Read First\n",
        "\n",
        "Welcome! This is the exact notebook used in the live demo.\n",
        "\n",
        "**How to use:**\n",
        "1. Go to `Runtime` ‚Üí `Run all`.\n",
        "2. Wait until all cells finish (you‚Äôll see ‚úÖ messages).\n",
        "3. Scroll and explore the outputs.\n",
        "\n",
        "> üîí **Don't worry:** This original version is read-only.\n",
        "> If you want to experiment, go to `File` ‚Üí `Save a copy in Drive` and play in *your* copy.\n"
      ],
      "metadata": {
        "id": "wf1Dv_F9cdKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Summarizer with Advanced Insights\n",
        "\n",
        "Turn raw datasets into **clear summaries, trends, drivers, and actionable insights** in just a few clicks.\n",
        "\n",
        "This notebook is designed for **non-technical users**.  \n",
        "You don‚Äôt need to edit any code.\n",
        "\n",
        "---\n",
        "\n",
        "## How this notebook works\n",
        "\n",
        "1. Click **Runtime ‚Üí Run all**\n",
        "2. Start by uploading one of the **Sample Datasets** (recommended)\n",
        "3. Select the **Dataset Type** (or keep it on Auto)\n",
        "4. Click **‚ñ∂ Generate insights**\n",
        "5. Review the insights across the tabs\n",
        "6. *(Optional)* Download the results\n",
        "\n",
        "That‚Äôs it.\n",
        "\n",
        "---\n",
        "\n",
        "## What happens behind the scenes\n",
        "\n",
        "When you run this notebook, it will automatically:\n",
        "\n",
        "- Prepare the required environment  \n",
        "- Load your uploaded dataset (or sample data)  \n",
        "- Validate data quality and structure  \n",
        "- Detect time, metrics, and dimensions automatically  \n",
        "- Show which insights are available (via capability badges)  \n",
        "- Generate:\n",
        "  - Executive summary  \n",
        "  - Trends over time (if applicable)  \n",
        "  - Key drivers and contributors  \n",
        "  - Anomaly signals (spikes and drops)  \n",
        "- *(Optional)* Prepare an AI-ready summary for narrative generation  \n",
        "\n",
        "If something needs attention, the notebook will show a **clear, human-readable message** explaining what‚Äôs missing or limited.\n",
        "\n",
        "---\n",
        "\n",
        "## First-time users (recommended)\n",
        "\n",
        "For your first run, use one of the **Sample Datasets** provided.\n",
        "\n",
        "This lets you:\n",
        "- See the full insight generation flow\n",
        "- Understand the outputs and tabs\n",
        "- Learn how dataset structure affects insights\n",
        "\n",
        "Once you‚Äôre comfortable, you can switch to **Upload Your Own Data** and re-run the analysis.\n",
        "\n",
        "---\n",
        "\n",
        "## Supported dataset types\n",
        "\n",
        "You can choose a dataset type to improve validation and insights:\n",
        "\n",
        "- **Business / Performance** ‚Äì sales, revenue, cost, operations  \n",
        "- **ESG / Impact** ‚Äì ESG scores or sustainability metrics  \n",
        "- **Startup / Deal Screening** ‚Äì traction, growth, early-stage metrics  \n",
        "- **Portfolio Monitoring** ‚Äì portfolio company performance  \n",
        "- **Generic (Auto)** ‚Äì auto-detect structure when unsure  \n",
        "\n",
        "If unsure, keep the selection on **Auto**.\n",
        "\n",
        "---\n",
        "\n",
        "## Supported input formats\n",
        "\n",
        "- CSV (`.csv`)\n",
        "- Excel (`.xlsx`, `.xls`)\n",
        "- Parquet (`.parquet`)\n",
        "\n",
        "---\n",
        "\n",
        "## Insights controls (left panel)\n",
        "\n",
        "You can fine-tune the analysis using simple controls:\n",
        "\n",
        "- **Anomaly sensitivity** ‚Äì how strict anomaly detection should be  \n",
        "- **Top-N drivers** ‚Äì how many top contributors to display  \n",
        "- **Enable downloads** ‚Äì allow or disable exporting results  \n",
        "\n",
        "If you‚Äôre not sure, the **default settings work well** for most datasets.\n",
        "\n",
        "---\n",
        "\n",
        "## About the AI (LLM) feature\n",
        "\n",
        "The AI narrative feature is **optional and disabled by default**.\n",
        "\n",
        "When disabled:\n",
        "- No external APIs are called  \n",
        "- All analysis runs locally inside the notebook  \n",
        "\n",
        "When enabled:\n",
        "- The AI generates a plain-English summary  \n",
        "- It uses **computed insights only**, not raw data  \n",
        "\n",
        "You remain fully in control.\n",
        "\n",
        "---\n",
        "\n",
        "## Tip\n",
        "\n",
        "This notebook is safe to explore.  \n",
        "If you want to try again, simply upload a new file or re-run with a sample dataset.\n",
        "\n",
        "No data is stored, and nothing is shared externally by default.\n"
      ],
      "metadata": {
        "id": "YlMNdBxJdluM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install & Imports"
      ],
      "metadata": {
        "id": "L-jBGg6Od3ml"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mnraVnDn5lpb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0caa9669-f881-4c48-a2eb-b98a79a8db71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.54.0-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Collecting cachetools<7,>=5.5 (from streamlit)\n",
            "  Downloading cachetools-6.2.6-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.46)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (26.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.26.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2026.1.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.54.0-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cachetools-6.2.6-py3-none-any.whl (11 kB)\n",
            "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cachetools, pydeck, streamlit\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 7.0.0\n",
            "    Uninstalling cachetools-7.0.0:\n",
            "      Successfully uninstalled cachetools-7.0.0\n",
            "Successfully installed cachetools-6.2.6 pydeck-0.9.1 streamlit-1.54.0\n",
            "‚úÖ Setup complete\n"
          ]
        }
      ],
      "source": [
        "# Install packages (once)\n",
        "# uncomment this if you run for the first time\n",
        "!pip install streamlit\n",
        "\n",
        "print(\"‚úÖ Setup complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Web App"
      ],
      "metadata": {
        "id": "cdrjqKjydz5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# app.py ‚Äî uPledge: Data Summarizer with Advanced Insights (V2.1)\n",
        "# Adds user-friendly descriptions for every major section + each output tab.\n",
        "# Run: streamlit run app.py\n",
        "\n",
        "import io\n",
        "import re\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "\n",
        "\n",
        "# ============================\n",
        "# Config\n",
        "# ============================\n",
        "ALLOWED_EXT = {\"csv\", \"xlsx\", \"xls\", \"parquet\"}\n",
        "MAX_ROWS_PREVIEW_DEFAULT = 200\n",
        "MAX_MB_SOFT = 60\n",
        "\n",
        "COMMON_DATE_NAMES = {\n",
        "    \"date\", \"month\", \"period\", \"week\", \"year\", \"time\", \"timestamp\",\n",
        "    \"dt\", \"order_date\", \"created_at\", \"start_date\", \"end_date\"\n",
        "}\n",
        "\n",
        "COMMON_METRIC_HINTS = {\n",
        "    \"revenue\", \"sales\", \"volume\", \"units\", \"profit\", \"margin\", \"cost\", \"spend\",\n",
        "    \"impressions\", \"clicks\", \"ctr\", \"cpc\", \"cpm\", \"roas\",\n",
        "    \"co2\", \"carbon\", \"emissions\", \"energy\", \"water\", \"waste\",\n",
        "    \"esg\", \"impact\", \"score\", \"rating\", \"users\", \"active\", \"retention\",\n",
        "    \"valuation\", \"irr\", \"multiple\", \"ownership\"\n",
        "}\n",
        "\n",
        "DATASET_OPTIONS = [\n",
        "    \"Auto\",\n",
        "    \"Business/Performance\",\n",
        "    \"ESG/Impact\",\n",
        "    \"Startup/Deal Screening\",\n",
        "    \"Portfolio Monitoring\",\n",
        "    \"Generic\",\n",
        "]\n",
        "\n",
        "DATASET_RULES = {\n",
        "    \"Business/Performance\": {\n",
        "        \"time_candidates\": [\"date\", \"month\", \"period\", \"week\"],\n",
        "        \"entity_candidates\": [\"company\", \"brand\", \"product\", \"customer\"],\n",
        "        \"metric_keywords\": [\"revenue\", \"sales\", \"profit\", \"cost\", \"units\", \"volume\", \"spend\"],\n",
        "        \"dims_keywords\": [\"region\", \"product\", \"channel\", \"category\", \"segment\", \"brand\"],\n",
        "    },\n",
        "    \"ESG/Impact\": {\n",
        "        \"time_candidates\": [\"year\", \"date\", \"period\"],\n",
        "        \"entity_candidates\": [\"company\", \"issuer\", \"entity\"],\n",
        "        \"metric_keywords\": [\"e_score\", \"s_score\", \"g_score\", \"esg\", \"impact\", \"co2\", \"emissions\", \"energy\", \"water\"],\n",
        "        \"dims_keywords\": [\"sector\", \"country\", \"pillar\", \"metric\", \"scope\"],\n",
        "    },\n",
        "    \"Startup/Deal Screening\": {\n",
        "        \"time_candidates\": [\"date\", \"month\", \"period\"],\n",
        "        \"entity_candidates\": [\"startup\", \"company\", \"venture\"],\n",
        "        \"metric_keywords\": [\"users\", \"revenue\", \"gmv\", \"mrr\", \"arr\", \"growth\", \"retention\", \"burn\", \"runway\"],\n",
        "        \"dims_keywords\": [\"sector\", \"country\", \"stage\", \"channel\", \"product\"],\n",
        "    },\n",
        "    \"Portfolio Monitoring\": {\n",
        "        \"time_candidates\": [\"period\", \"date\", \"year\", \"quarter\"],\n",
        "        \"entity_candidates\": [\"company\", \"portfolio_company\", \"asset\"],\n",
        "        \"metric_keywords\": [\"valuation\", \"revenue\", \"irr\", \"multiple\", \"ownership\", \"ebitda\"],\n",
        "        \"dims_keywords\": [\"fund\", \"sector\", \"region\", \"stage\", \"currency\"],\n",
        "    },\n",
        "    \"Generic\": {\n",
        "        \"time_candidates\": [\"date\", \"month\", \"period\", \"year\", \"week\"],\n",
        "        \"entity_candidates\": [],\n",
        "        \"metric_keywords\": [],\n",
        "        \"dims_keywords\": [],\n",
        "    },\n",
        "    \"Auto\": {\n",
        "        \"time_candidates\": [\"date\", \"month\", \"period\", \"year\", \"week\"],\n",
        "        \"entity_candidates\": [\"company\", \"startup\", \"product\", \"region\"],\n",
        "        \"metric_keywords\": [],\n",
        "        \"dims_keywords\": [],\n",
        "    },\n",
        "}\n",
        "\n",
        "\n",
        "# ============================\n",
        "# Utilities\n",
        "# ============================\n",
        "def _clean_colname(c: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", str(c)).strip()\n",
        "\n",
        "\n",
        "def _file_ext(filename: str) -> str:\n",
        "    return filename.lower().split(\".\")[-1].strip()\n",
        "\n",
        "\n",
        "def _bytes_to_mb(n: int) -> float:\n",
        "    return round(n / (1024 * 1024), 2)\n",
        "\n",
        "\n",
        "def _to_numeric(s: pd.Series) -> pd.Series:\n",
        "    if pd.api.types.is_numeric_dtype(s):\n",
        "        return s\n",
        "    return pd.to_numeric(s.astype(str).str.replace(\",\", \"\").str.strip(), errors=\"coerce\")\n",
        "\n",
        "\n",
        "def _try_parse_datetime_series(s: pd.Series) -> Tuple[pd.Series, float]:\n",
        "    raw = s.astype(str).str.strip()\n",
        "    raw = raw.replace({\"\": np.nan, \"nan\": np.nan, \"None\": np.nan, \"NaT\": np.nan})\n",
        "    non_empty = raw.notna().sum()\n",
        "    if non_empty == 0:\n",
        "        return pd.to_datetime(pd.Series([pd.NaT] * len(s))), 0.0\n",
        "\n",
        "    p1 = pd.to_datetime(raw, errors=\"coerce\", infer_datetime_format=True)\n",
        "    r1 = p1.notna().sum() / non_empty\n",
        "\n",
        "    p2 = pd.to_datetime(raw, errors=\"coerce\", format=\"%m-%Y\")\n",
        "    r2 = p2.notna().sum() / non_empty\n",
        "\n",
        "    p3 = pd.to_datetime(raw, errors=\"coerce\", format=\"%Y-%m\")\n",
        "    r3 = p3.notna().sum() / non_empty\n",
        "\n",
        "    parsed, ratio = p1, r1\n",
        "    for p, r in [(p2, r2), (p3, r3)]:\n",
        "        if r > ratio:\n",
        "            parsed, ratio = p, r\n",
        "\n",
        "    return parsed, float(ratio)\n",
        "\n",
        "\n",
        "def _safe_to_datetime(s: pd.Series) -> pd.Series:\n",
        "    parsed, ratio = _try_parse_datetime_series(s)\n",
        "    return parsed if ratio >= 0.4 else pd.to_datetime(s, errors=\"coerce\")\n",
        "\n",
        "\n",
        "def _looks_like_id(col: pd.Series) -> bool:\n",
        "    n = len(col)\n",
        "    if n < 30:\n",
        "        return False\n",
        "    nunique = col.nunique(dropna=True)\n",
        "    if nunique == 0:\n",
        "        return False\n",
        "    return (nunique / max(1, n)) >= 0.85\n",
        "\n",
        "\n",
        "def _is_mostly_numeric(col: pd.Series) -> bool:\n",
        "    if pd.api.types.is_numeric_dtype(col):\n",
        "        return True\n",
        "    coerced = pd.to_numeric(col.astype(str).str.replace(\",\", \"\").str.strip(), errors=\"coerce\")\n",
        "    return coerced.notna().mean() >= 0.85\n",
        "\n",
        "\n",
        "def _coerce_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    for c in out.columns:\n",
        "        if pd.api.types.is_numeric_dtype(out[c]):\n",
        "            continue\n",
        "        if out[c].dtype == \"object\":\n",
        "            s = out[c].astype(str).str.replace(\",\", \"\").str.strip()\n",
        "            coerced = pd.to_numeric(s, errors=\"coerce\")\n",
        "            if coerced.notna().mean() >= 0.90:\n",
        "                out[c] = coerced\n",
        "    return out\n",
        "\n",
        "\n",
        "def _infer_grain_from_dates(dt: pd.Series) -> str:\n",
        "    u = dt.dropna().sort_values().unique()\n",
        "    if len(u) < 3:\n",
        "        return \"unknown\"\n",
        "    deltas = np.diff(u).astype(\"timedelta64[D]\").astype(int)\n",
        "    if len(deltas) == 0:\n",
        "        return \"unknown\"\n",
        "    med = int(np.median(deltas))\n",
        "    if med <= 2:\n",
        "        return \"daily\"\n",
        "    if med <= 10:\n",
        "        return \"weekly-ish\"\n",
        "    if med <= 40:\n",
        "        return \"monthly-ish\"\n",
        "    if med <= 120:\n",
        "        return \"quarterly-ish\"\n",
        "    return \"yearly-ish\"\n",
        "\n",
        "\n",
        "def _format_pct(x: float) -> str:\n",
        "    if x is None or (isinstance(x, float) and (np.isnan(x) or np.isinf(x))):\n",
        "        return \"-\"\n",
        "    return f\"{x * 100:.1f}%\"\n",
        "\n",
        "\n",
        "def _ranked_contribution(df: pd.DataFrame, seg_col: str, metric_col: str) -> pd.DataFrame:\n",
        "    tmp = df.groupby(seg_col, dropna=False)[metric_col].sum().sort_values(ascending=False)\n",
        "    out = tmp.reset_index().rename(columns={metric_col: \"value\"})\n",
        "    total = out[\"value\"].sum()\n",
        "    out[\"share\"] = out[\"value\"] / total if total != 0 else np.nan\n",
        "    out[\"cum_share\"] = out[\"share\"].cumsum()\n",
        "    return out\n",
        "\n",
        "\n",
        "def _detect_anomalies(values: pd.Series, z_thresh: float = 3.0) -> pd.DataFrame:\n",
        "    x = values.astype(float).copy()\n",
        "    med = np.nanmedian(x)\n",
        "    mad = np.nanmedian(np.abs(x - med))\n",
        "    if mad == 0 or np.isnan(mad):\n",
        "        return pd.DataFrame(columns=[\"idx\", \"value\", \"score\", \"type\"])\n",
        "    robust_z = 0.6745 * (x - med) / mad\n",
        "    idx = np.where(np.abs(robust_z) >= z_thresh)[0]\n",
        "    out = pd.DataFrame({\"idx\": idx, \"value\": x.iloc[idx].values, \"score\": robust_z.iloc[idx].values})\n",
        "    out[\"type\"] = np.where(out[\"score\"] > 0, \"spike\", \"drop\")\n",
        "    return out\n",
        "\n",
        "\n",
        "def _make_insight(priority: int, title: str, evidence: str, action: str, tags: List[str]) -> dict:\n",
        "    return {\n",
        "        \"Priority\": priority,\n",
        "        \"Insight\": title,\n",
        "        \"Evidence\": evidence,\n",
        "        \"Suggested action\": action,\n",
        "        \"Tags\": \", \".join(tags),\n",
        "    }\n",
        "\n",
        "\n",
        "def _read_uploaded_file(file) -> pd.DataFrame:\n",
        "    ext = _file_ext(file.name)\n",
        "    if ext not in ALLOWED_EXT:\n",
        "        raise ValueError(f\"Unsupported file type: .{ext}\")\n",
        "\n",
        "    if ext == \"csv\":\n",
        "        content = file.getvalue()\n",
        "        for enc in [\"utf-8\", \"utf-8-sig\", \"latin1\"]:\n",
        "            try:\n",
        "                return pd.read_csv(io.BytesIO(content), encoding=enc)\n",
        "            except Exception:\n",
        "                continue\n",
        "        return pd.read_csv(io.BytesIO(content))\n",
        "\n",
        "    if ext in {\"xlsx\", \"xls\"}:\n",
        "        return pd.read_excel(file)\n",
        "\n",
        "    if ext == \"parquet\":\n",
        "        return pd.read_parquet(file)\n",
        "\n",
        "    raise ValueError(\"Unsupported file type.\")\n",
        "\n",
        "\n",
        "# ============================\n",
        "# Validation + Capabilities\n",
        "# ============================\n",
        "@dataclass\n",
        "class CapabilityFlags:\n",
        "    trend_ready: bool\n",
        "    segment_ready: bool\n",
        "    anomaly_ready: bool\n",
        "    driver_ready: bool\n",
        "    investor_ready: bool\n",
        "    llm_ready: bool\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ValidationResult:\n",
        "    ok: bool\n",
        "    errors: List[str]\n",
        "    warnings: List[str]\n",
        "    info: List[str]\n",
        "    detected_date_cols: List[str]\n",
        "    detected_metric_cols: List[str]\n",
        "    detected_dimension_cols: List[str]\n",
        "    suspected_id_cols: List[str]\n",
        "    health_score: int\n",
        "    capabilities: CapabilityFlags\n",
        "    dataset_type: str\n",
        "\n",
        "\n",
        "def _detect_cols_with_keywords(cols: List[str], keywords: List[str]) -> List[str]:\n",
        "    if not keywords:\n",
        "        return []\n",
        "    out = []\n",
        "    for c in cols:\n",
        "        cl = c.lower().replace(\" \", \"_\")\n",
        "        if any(k in cl for k in keywords):\n",
        "            out.append(c)\n",
        "    return out\n",
        "\n",
        "\n",
        "def _profile_validate_and_capabilities(df: pd.DataFrame, dataset_type: str) -> ValidationResult:\n",
        "    errors, warnings, info = [], [], []\n",
        "\n",
        "    if df is None or df.empty:\n",
        "        caps = CapabilityFlags(False, False, False, False, False, False)\n",
        "        return ValidationResult(\n",
        "            ok=False,\n",
        "            errors=[\"Dataset is empty.\"],\n",
        "            warnings=[],\n",
        "            info=[],\n",
        "            detected_date_cols=[],\n",
        "            detected_metric_cols=[],\n",
        "            detected_dimension_cols=[],\n",
        "            suspected_id_cols=[],\n",
        "            health_score=0,\n",
        "            capabilities=caps,\n",
        "            dataset_type=dataset_type,\n",
        "        )\n",
        "\n",
        "    df = df.copy()\n",
        "    df.columns = [_clean_colname(c) for c in df.columns]\n",
        "    cols = list(df.columns)\n",
        "\n",
        "    n_rows, n_cols = df.shape\n",
        "    info.append(f\"Rows: {n_rows:,} | Columns: {n_cols:,}\")\n",
        "\n",
        "    df2 = _coerce_numeric(df)\n",
        "\n",
        "    missing_pct = (df2.isna().mean() * 100).sort_values(ascending=False)\n",
        "    high_missing = missing_pct[missing_pct > 60]\n",
        "    if len(high_missing) > 0:\n",
        "        warnings.append(f\"{len(high_missing)} columns have >60% missing values (may weaken insights).\")\n",
        "\n",
        "    dup_count = int(df2.duplicated().sum())\n",
        "    if dup_count > 0:\n",
        "        warnings.append(f\"Detected {dup_count:,} duplicate rows (consider deduping).\")\n",
        "\n",
        "    rules = DATASET_RULES.get(dataset_type, DATASET_RULES[\"Generic\"])\n",
        "\n",
        "    # Detect time/date columns\n",
        "    detected_date_cols: List[str] = []\n",
        "    date_ratios: Dict[str, float] = {}\n",
        "    preferred_time = set([c.lower() for c in rules.get(\"time_candidates\", [])])\n",
        "\n",
        "    for c in cols:\n",
        "        cname = c.lower().strip()\n",
        "        name_hint = (\n",
        "            cname in COMMON_DATE_NAMES\n",
        "            or \"date\" in cname\n",
        "            or \"month\" in cname\n",
        "            or \"period\" in cname\n",
        "            or \"time\" in cname\n",
        "            or cname in preferred_time\n",
        "        )\n",
        "\n",
        "        if name_hint:\n",
        "            parsed, ratio = _try_parse_datetime_series(df2[c])\n",
        "            date_ratios[c] = ratio\n",
        "            if ratio >= 0.70:\n",
        "                detected_date_cols.append(c)\n",
        "            continue\n",
        "\n",
        "        if df2[c].dtype == \"object\":\n",
        "            sample = df2[c].dropna().astype(str).str.strip().head(50)\n",
        "            if not sample.empty:\n",
        "                patt_score = sample.str.contains(r\"\\d{1,4}[-/]\\d{1,2}([-/]\\d{1,2})?\").mean()\n",
        "                if patt_score >= 0.6:\n",
        "                    parsed, ratio = _try_parse_datetime_series(df2[c])\n",
        "                    date_ratios[c] = ratio\n",
        "                    if ratio >= 0.70:\n",
        "                        detected_date_cols.append(c)\n",
        "\n",
        "    numeric_cols = [c for c in cols if pd.api.types.is_numeric_dtype(df2[c]) or _is_mostly_numeric(df2[c])]\n",
        "\n",
        "    metric_by_keywords = _detect_cols_with_keywords(numeric_cols, rules.get(\"metric_keywords\", []))\n",
        "    metric_by_common = _detect_cols_with_keywords(numeric_cols, list(COMMON_METRIC_HINTS))\n",
        "\n",
        "    detected_metric_cols: List[str] = []\n",
        "    detected_metric_cols.extend(metric_by_keywords)\n",
        "    for c in metric_by_common:\n",
        "        if c not in detected_metric_cols:\n",
        "            detected_metric_cols.append(c)\n",
        "\n",
        "    if len(detected_metric_cols) == 0 and len(numeric_cols) > 0:\n",
        "        detected_metric_cols = numeric_cols[: min(10, len(numeric_cols))]\n",
        "        warnings.append(\"No obvious metric columns by name; using numeric columns as metric candidates.\")\n",
        "\n",
        "    suspected_id_cols: List[str] = []\n",
        "    detected_dimension_cols: List[str] = []\n",
        "    for c in cols:\n",
        "        if c in detected_date_cols or c in numeric_cols:\n",
        "            continue\n",
        "        if _looks_like_id(df2[c]):\n",
        "            suspected_id_cols.append(c)\n",
        "            continue\n",
        "        nunique = df2[c].nunique(dropna=True)\n",
        "        if nunique <= 200 or (nunique / max(1, n_rows)) <= 0.05:\n",
        "            detected_dimension_cols.append(c)\n",
        "\n",
        "    # Dataset-specific soft warning: missing entity column for certain types\n",
        "    entity_candidates = rules.get(\"entity_candidates\", [])\n",
        "    entity_found = False\n",
        "    if entity_candidates:\n",
        "        entity_found = any(any(ec in c.lower().replace(\" \", \"_\") for ec in entity_candidates) for c in cols)\n",
        "\n",
        "    time_found = len(detected_date_cols) > 0\n",
        "    metric_found = len(detected_metric_cols) > 0\n",
        "    dim_found = len(detected_dimension_cols) > 0\n",
        "\n",
        "    if not metric_found:\n",
        "        errors.append(\"No usable numeric metric column detected (needed for insights).\")\n",
        "\n",
        "    if dataset_type in {\"ESG/Impact\", \"Portfolio Monitoring\", \"Startup/Deal Screening\"} and not entity_found:\n",
        "        warnings.append(\"No clear entity column detected (e.g., company/startup). Insights may be less meaningful.\")\n",
        "\n",
        "    # Health score\n",
        "    score = 100\n",
        "    score -= int(min(40, missing_pct.mean()))\n",
        "    if dup_count > 0:\n",
        "        score -= 5\n",
        "    if len(high_missing) > 0:\n",
        "        score -= 10\n",
        "    if not time_found:\n",
        "        score -= 10\n",
        "    if not dim_found:\n",
        "        score -= 10\n",
        "    if not metric_found:\n",
        "        score -= 40\n",
        "    score = max(0, min(100, score))\n",
        "\n",
        "    if time_found:\n",
        "        best = sorted(detected_date_cols, key=lambda c: date_ratios.get(c, 0), reverse=True)[0]\n",
        "        parsed, _ = _try_parse_datetime_series(df2[best])\n",
        "        if parsed.notna().any():\n",
        "            info.append(f\"Best time column: '{best}' | Range: {parsed.min().date()} ‚Üí {parsed.max().date()}\")\n",
        "\n",
        "    info.append(f\"Health score: {score}/100\")\n",
        "\n",
        "    caps = CapabilityFlags(\n",
        "        trend_ready=bool(time_found and metric_found),\n",
        "        segment_ready=bool(dim_found),\n",
        "        anomaly_ready=bool(time_found and metric_found),\n",
        "        driver_ready=bool(dim_found and metric_found),\n",
        "        investor_ready=bool(metric_found and (time_found or dim_found) and score >= 60),\n",
        "        llm_ready=True,\n",
        "    )\n",
        "\n",
        "    return ValidationResult(\n",
        "        ok=(len(errors) == 0),\n",
        "        errors=errors,\n",
        "        warnings=warnings,\n",
        "        info=info,\n",
        "        detected_date_cols=detected_date_cols,\n",
        "        detected_metric_cols=detected_metric_cols,\n",
        "        detected_dimension_cols=detected_dimension_cols,\n",
        "        suspected_id_cols=suspected_id_cols,\n",
        "        health_score=score,\n",
        "        capabilities=caps,\n",
        "        dataset_type=dataset_type,\n",
        "    )\n",
        "\n",
        "\n",
        "# ============================\n",
        "# LLM Placeholder\n",
        "# ============================\n",
        "def build_llm_prompt(summary_payload: dict) -> str:\n",
        "    return f\"\"\"\n",
        "You are an analyst for uPledge. Write a clear narrative based ONLY on the provided summary JSON.\n",
        "Do not invent data. If something is missing, say \"unknown\".\n",
        "\n",
        "Return:\n",
        "1) Executive summary (5 bullets)\n",
        "2) Key insights (max 8)\n",
        "3) Risks / data gaps (max 6)\n",
        "4) Suggested next actions (max 6)\n",
        "5) Questions to validate (max 6)\n",
        "\n",
        "SUMMARY_JSON:\n",
        "{summary_payload}\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def call_llm_placeholder(provider: str, api_key: str, prompt: str) -> str:\n",
        "    _ = (provider, api_key, prompt)\n",
        "    return (\n",
        "        \"LLM Narrator (placeholder)\\n\\n\"\n",
        "        \"Executive summary:\\n\"\n",
        "        \"- (Placeholder) Connect Gemini/OpenAI to generate real narrative.\\n\\n\"\n",
        "        \"Key insights:\\n\"\n",
        "        \"- Insight 1...\\n\\n\"\n",
        "        \"Risks / data gaps:\\n\"\n",
        "        \"- Gap 1...\\n\\n\"\n",
        "        \"Suggested next actions:\\n\"\n",
        "        \"- Action 1...\\n\\n\"\n",
        "        \"Questions to validate:\\n\"\n",
        "        \"- Question 1...\\n\"\n",
        "    )\n",
        "\n",
        "\n",
        "# ============================\n",
        "# UI Helpers (Descriptions)\n",
        "# ============================\n",
        "def section_help(title: str, text: str):\n",
        "    st.markdown(f\"**{title}**\")\n",
        "    st.caption(text)\n",
        "\n",
        "\n",
        "def badge_help():\n",
        "    with st.expander(\"What do these badges mean?\", expanded=False):\n",
        "        st.markdown(\n",
        "            \"\"\"\n",
        "- **Trend-ready**: time + metric exists ‚Üí trends & growth can be computed\n",
        "- **Driver-ready**: dimensions + metric ‚Üí top contributors & movers\n",
        "- **Segment-ready**: at least 1 dimension column detected\n",
        "- **Anomaly-ready**: time + metric ‚Üí spikes/drops can be flagged\n",
        "- **Investor-ready**: dataset is structured enough for decision-style summary (not just exploratory)\n",
        "- **LLM-ready**: narrative generation is possible if you enable it (uses computed summaries only)\n",
        "\"\"\"\n",
        "        )\n",
        "\n",
        "\n",
        "# ============================\n",
        "# Streamlit App\n",
        "# ============================\n",
        "st.set_page_config(page_title=\"uPledge ‚Äî Data Summarizer (V2.1)\", layout=\"wide\")\n",
        "st.title(\"uPledge ‚Äî Data Summarizer with Advanced Insights (V2.1)\")\n",
        "st.caption(\"More user-friendly: each section includes a short explanation.\")\n",
        "\n",
        "with st.expander(\"Supported inputs (V2.1)\", expanded=False):\n",
        "    st.markdown(\n",
        "        \"\"\"\n",
        "**Best inputs**\n",
        "- CSV / Excel / Parquet (structured tables)\n",
        "- Has: time column (date/month/period) + numeric metric (sales/revenue/score)\n",
        "- Optional: dimensions (region/product/company/sector/channel)\n",
        "\n",
        "**Not supported (yet)**\n",
        "- PDFs, images, scanned reports, free-text documents (separate advanced tier)\n",
        "\"\"\"\n",
        "    )\n",
        "\n",
        "# ---- Upload\n",
        "section_help(\n",
        "    \"Upload dataset\",\n",
        "    \"Upload a CSV/Excel/Parquet file. We‚Äôll preview it, check data health, then generate insights based on your selections.\",\n",
        ")\n",
        "\n",
        "colA, colB = st.columns([2, 1], gap=\"large\")\n",
        "with colA:\n",
        "    uploaded = st.file_uploader(\"File upload\", type=list(ALLOWED_EXT), accept_multiple_files=False)\n",
        "with colB:\n",
        "    section_help(\"Dataset type\", \"This helps uPledge apply better expectations (ESG vs business vs startup).\")\n",
        "    dataset_kind = st.selectbox(\"Dataset type\", DATASET_OPTIONS, index=0)\n",
        "    strict_time = st.toggle(\"Strict: require a time column (for trends)\", value=False)\n",
        "    preview_rows = st.slider(\"Preview rows\", 20, 500, MAX_ROWS_PREVIEW_DEFAULT, step=10)\n",
        "\n",
        "st.divider()\n",
        "\n",
        "if not uploaded:\n",
        "    st.info(\"Upload a file to begin.\")\n",
        "    st.stop()\n",
        "\n",
        "# File size warning\n",
        "try:\n",
        "    size_mb = _bytes_to_mb(len(uploaded.getvalue()))\n",
        "    if size_mb > MAX_MB_SOFT:\n",
        "        st.warning(f\"File size is {size_mb} MB. If it feels slow, try a smaller export or Parquet.\")\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Read\n",
        "try:\n",
        "    df_in = _read_uploaded_file(uploaded)\n",
        "except Exception as e:\n",
        "    st.error(f\"Failed to read file: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "# Preview\n",
        "section_help(\"Preview\", \"Quickly confirm your data is uploaded correctly (columns align, values look right).\")\n",
        "\n",
        "left, right = st.columns([2, 1], gap=\"large\")\n",
        "with left:\n",
        "    st.dataframe(df_in.head(preview_rows), use_container_width=True)\n",
        "with right:\n",
        "    section_help(\"Quick stats\", \"Basic dataset size + a column list to spot missing key columns.\")\n",
        "    st.write(f\"**Rows:** {df_in.shape[0]:,}\")\n",
        "    st.write(f\"**Columns:** {df_in.shape[1]:,}\")\n",
        "    st.write(\"**Columns list:**\")\n",
        "    st.code(\", \".join([str(c) for c in df_in.columns[:60]]) + (\" ...\" if df_in.shape[1] > 60 else \"\"))\n",
        "\n",
        "st.divider()\n",
        "\n",
        "# Validate + Capabilities\n",
        "section_help(\n",
        "    \"Validation + capability badges\",\n",
        "    \"We check data health and show what uPledge can reliably compute (trends, drivers, anomalies).\",\n",
        ")\n",
        "\n",
        "vr = _profile_validate_and_capabilities(df_in, dataset_kind)\n",
        "caps = vr.capabilities\n",
        "\n",
        "b1, b2, b3, b4 = st.columns([1, 1, 1, 1])\n",
        "b1.metric(\"Health score\", f\"{vr.health_score}/100\")\n",
        "b2.metric(\"Time cols\", str(len(vr.detected_date_cols)))\n",
        "b3.metric(\"Metric cols\", str(len(vr.detected_metric_cols)))\n",
        "b4.metric(\"Dimensions\", str(len(vr.detected_dimension_cols)))\n",
        "\n",
        "badge_cols = st.columns(6)\n",
        "badge_cols[0].metric(\"Trend-ready\", \"‚úÖ\" if caps.trend_ready else \"‚ùå\")\n",
        "badge_cols[1].metric(\"Driver-ready\", \"‚úÖ\" if caps.driver_ready else \"‚ùå\")\n",
        "badge_cols[2].metric(\"Segment-ready\", \"‚úÖ\" if caps.segment_ready else \"‚ùå\")\n",
        "badge_cols[3].metric(\"Anomaly-ready\", \"‚úÖ\" if caps.anomaly_ready else \"‚ùå\")\n",
        "badge_cols[4].metric(\"Investor-ready\", \"‚úÖ\" if caps.investor_ready else \"‚ùå\")\n",
        "badge_cols[5].metric(\"LLM-ready\", \"‚úÖ\" if caps.llm_ready else \"‚ùå\")\n",
        "badge_help()\n",
        "\n",
        "for s in vr.info:\n",
        "    st.write(\"‚Ä¢ \" + s)\n",
        "\n",
        "if vr.suspected_id_cols:\n",
        "    st.caption(\"Suspected ID columns (excluded from dimensions): \" + \", \".join(vr.suspected_id_cols))\n",
        "\n",
        "if vr.errors:\n",
        "    st.error(\"Blocking issues (must fix):\")\n",
        "    for e in vr.errors:\n",
        "        st.write(\"‚Ä¢ \" + e)\n",
        "\n",
        "if vr.warnings:\n",
        "    st.warning(\"Warnings (insights may be limited):\")\n",
        "    for w in vr.warnings:\n",
        "        st.write(\"‚Ä¢ \" + w)\n",
        "\n",
        "if strict_time and not caps.trend_ready:\n",
        "    st.error(\"Strict mode is ON: a usable time/date/period column is required.\")\n",
        "    st.stop()\n",
        "\n",
        "if not vr.ok:\n",
        "    st.info(\"Fix the blocking issues, then re-upload (or export a cleaned version).\")\n",
        "    st.stop()\n",
        "\n",
        "st.divider()\n",
        "\n",
        "# Schema mapping\n",
        "section_help(\n",
        "    \"Schema mapping (override)\",\n",
        "    \"Select which columns represent time, your main metric, and the segments (dimensions) to compare.\",\n",
        ")\n",
        "\n",
        "c1, c2, c3 = st.columns(3)\n",
        "with c1:\n",
        "    date_col = st.selectbox(\n",
        "        \"Time column\",\n",
        "        options=[\"(none)\"] + vr.detected_date_cols,\n",
        "        index=1 if vr.detected_date_cols else 0,\n",
        "    )\n",
        "with c2:\n",
        "    metric_col = st.selectbox(\n",
        "        \"Primary metric (the number you want to explain)\",\n",
        "        options=vr.detected_metric_cols if vr.detected_metric_cols else list(df_in.columns),\n",
        "        index=0,\n",
        "    )\n",
        "with c3:\n",
        "    dims_default = vr.detected_dimension_cols[:5]\n",
        "    dimensions = st.multiselect(\n",
        "        \"Dimensions (segments to compare)\",\n",
        "        options=[c for c in df_in.columns if c != metric_col],\n",
        "        default=dims_default,\n",
        "    )\n",
        "\n",
        "st.caption(\"Example: Time=date, Metric=revenue, Dimensions=region/product/company/sector\")\n",
        "\n",
        "st.divider()\n",
        "\n",
        "# LLM narrator (placeholder)\n",
        "section_help(\n",
        "    \"LLM narrator (optional)\",\n",
        "    \"If enabled, the LLM writes a plain-English narrative using computed summaries only (not raw rows).\",\n",
        ")\n",
        "llm_on = st.toggle(\"Enable LLM narrator\", value=False)\n",
        "llm_provider = st.selectbox(\"Provider\", [\"ChatGPT (OpenAI)\", \"Gemini (Google)\"], index=0, disabled=not llm_on)\n",
        "llm_key = st.text_input(\"API Key\", type=\"password\", disabled=not llm_on)\n",
        "\n",
        "st.divider()\n",
        "\n",
        "run_insights = st.button(\"Generate insights\", type=\"primary\")\n",
        "\n",
        "st.session_state[\"upledge_df\"] = df_in\n",
        "st.session_state[\"upledge_schema\"] = {\n",
        "    \"dataset_kind\": dataset_kind,\n",
        "    \"date_col\": None if date_col == \"(none)\" else date_col,\n",
        "    \"metric_col\": metric_col,\n",
        "    \"dimensions\": dimensions,\n",
        "    \"validation\": asdict(vr),\n",
        "    \"caps\": asdict(vr.capabilities),\n",
        "    \"llm\": {\"enabled\": llm_on, \"provider\": llm_provider, \"has_key\": bool(llm_key)},\n",
        "}\n",
        "\n",
        "if not run_insights:\n",
        "    st.stop()\n",
        "\n",
        "# ============================\n",
        "# Insights Engine\n",
        "# ============================\n",
        "st.header(\"Insights Engine\")\n",
        "st.caption(\"This section generates insights based on the schema mapping above.\")\n",
        "\n",
        "df = df_in.copy()\n",
        "date_col = st.session_state[\"upledge_schema\"][\"date_col\"]\n",
        "metric_col = st.session_state[\"upledge_schema\"][\"metric_col\"]\n",
        "dimensions = st.session_state[\"upledge_schema\"][\"dimensions\"]\n",
        "\n",
        "if metric_col not in df.columns:\n",
        "    st.error(\"Selected metric column not found. Please reselect schema.\")\n",
        "    st.stop()\n",
        "\n",
        "df.columns = [str(c).strip() for c in df.columns]\n",
        "df[metric_col] = _to_numeric(df[metric_col])\n",
        "df = df.dropna(subset=[metric_col])\n",
        "\n",
        "if date_col and date_col in df.columns:\n",
        "    df[date_col] = _safe_to_datetime(df[date_col])\n",
        "\n",
        "with st.sidebar:\n",
        "    st.subheader(\"Insights controls\")\n",
        "    st.caption(\"Tune sensitivity and top-N results.\")\n",
        "    z_thresh = st.slider(\"Anomaly sensitivity (Z threshold)\", 2.0, 6.0, 3.0, 0.5)\n",
        "    top_n = st.slider(\"Top-N for drivers\", 3, 20, 8, 1)\n",
        "    enable_downloads = st.toggle(\"Enable downloads\", value=True)\n",
        "\n",
        "tab1, tab2, tab3, tab4, tab5 = st.tabs(\n",
        "    [\"Executive Summary\", \"Trends\", \"Drivers\", \"Anomalies\", \"LLM Narrative\"]\n",
        ")\n",
        "\n",
        "# Executive Summary\n",
        "with tab1:\n",
        "    section_help(\n",
        "        \"Executive Summary\",\n",
        "        \"A quick, factual overview: dataset size, missingness, totals, and time coverage (if available).\",\n",
        "    )\n",
        "\n",
        "    n_rows, n_cols = df_in.shape\n",
        "    missing_overall = float(df_in.isna().mean().mean())\n",
        "    metric_missing = float(df_in[metric_col].isna().mean()) if metric_col in df_in.columns else np.nan\n",
        "    metric_total = float(df[metric_col].sum()) if len(df) else np.nan\n",
        "\n",
        "    st.write(f\"‚Ä¢ Dataset type: **{dataset_kind}**\")\n",
        "    st.write(f\"‚Ä¢ Rows: **{n_rows:,}** | Columns: **{n_cols:,}**\")\n",
        "    st.write(f\"‚Ä¢ Missingness overall: **{missing_overall*100:.1f}%** | '{metric_col}' missing: **{metric_missing*100:.1f}%**\")\n",
        "    if not np.isnan(metric_total):\n",
        "        st.write(f\"‚Ä¢ Total '{metric_col}': **{metric_total:,.2f}**\")\n",
        "\n",
        "    if caps.trend_ready and date_col and df[date_col].notna().any():\n",
        "        dt = df[date_col].dropna()\n",
        "        st.write(f\"‚Ä¢ Time coverage: **{dt.min().date()} ‚Üí {dt.max().date()}**\")\n",
        "        st.write(f\"‚Ä¢ Time grain: **{_infer_grain_from_dates(dt)}**\")\n",
        "    else:\n",
        "        st.write(\"‚Ä¢ Trends are disabled (no usable time column selected).\")\n",
        "\n",
        "# Trends\n",
        "with tab2:\n",
        "    section_help(\n",
        "        \"Trends\",\n",
        "        \"Shows how your metric changes over time (plus a smoothed rolling average). Requires a time column + metric.\",\n",
        "    )\n",
        "\n",
        "    if not (caps.trend_ready and date_col and date_col in df.columns and df[date_col].notna().any()):\n",
        "        st.info(\"Trend-ready is ‚ùå ‚Äî trends are disabled.\")\n",
        "    else:\n",
        "        ts = df.groupby(date_col)[metric_col].sum().sort_index()\n",
        "        ts_df = ts.reset_index().rename(columns={date_col: \"date\", metric_col: \"value\"})\n",
        "        ts_df[\"value\"] = ts_df[\"value\"].astype(float)\n",
        "        ts_df[\"pct_change\"] = ts_df[\"value\"].pct_change()\n",
        "        ts_df[\"rolling_mean_3\"] = ts_df[\"value\"].rolling(3, min_periods=1).mean()\n",
        "\n",
        "        if len(ts_df) >= 2:\n",
        "            latest = ts_df[\"value\"].iloc[-1]\n",
        "            prev = ts_df[\"value\"].iloc[-2]\n",
        "            mom = (latest - prev) / prev if prev not in (0, np.nan) else np.nan\n",
        "            st.write(f\"Latest: **{latest:,.2f}** | Previous: **{prev:,.2f}** | Change: **{_format_pct(mom)}**\")\n",
        "\n",
        "        st.line_chart(ts_df.set_index(\"date\")[[\"value\"]])\n",
        "        st.line_chart(ts_df.set_index(\"date\")[[\"rolling_mean_3\"]])\n",
        "\n",
        "        with st.expander(\"Show trend table\"):\n",
        "            show = ts_df.copy()\n",
        "            show[\"pct_change\"] = show[\"pct_change\"].apply(lambda x: _format_pct(x) if pd.notna(x) else \"-\")\n",
        "            st.dataframe(show, use_container_width=True)\n",
        "\n",
        "# Drivers\n",
        "with tab3:\n",
        "    section_help(\n",
        "        \"Drivers\",\n",
        "        \"Ranks which segments contribute most to the metric (and what changed most recently if time exists). Requires dimensions + metric.\",\n",
        "    )\n",
        "\n",
        "    driver_insights: List[dict] = []\n",
        "\n",
        "    if not caps.driver_ready or not dimensions:\n",
        "        st.info(\"Driver-ready is ‚ùå or no dimensions selected ‚Äî driver analysis is limited.\")\n",
        "    else:\n",
        "        for dim in dimensions:\n",
        "            if dim not in df.columns:\n",
        "                continue\n",
        "\n",
        "            st.markdown(f\"### Dimension: `{dim}`\")\n",
        "            st.caption(\"Contribution table: value = total, share = % of total, cum_share = cumulative %.\")\n",
        "            contrib = _ranked_contribution(df, dim, metric_col).head(top_n)\n",
        "            st.dataframe(contrib, use_container_width=True)\n",
        "\n",
        "            if len(contrib) >= 3 and contrib[\"cum_share\"].iloc[-1] >= 0.8:\n",
        "                top_name = str(contrib[dim].iloc[0])\n",
        "                top_share = float(contrib[\"share\"].iloc[0])\n",
        "                driver_insights.append(\n",
        "                    _make_insight(\n",
        "                        2,\n",
        "                        f\"High dependence on {dim}='{top_name}'\",\n",
        "                        f\"Top {dim} contributes {_format_pct(top_share)} of '{metric_col}' (top {len(contrib)} contribute {contrib['cum_share'].iloc[-1]*100:.1f}%).\",\n",
        "                        f\"Check concentration risk; consider balancing across other {dim} if needed.\",\n",
        "                        [\"concentration\", \"risk\", dim],\n",
        "                    )\n",
        "                )\n",
        "\n",
        "            if caps.trend_ready and date_col and date_col in df.columns and df[date_col].notna().any():\n",
        "                tmp = df.dropna(subset=[date_col]).copy()\n",
        "                if tmp[date_col].nunique() >= 2:\n",
        "                    last_date = tmp[date_col].max()\n",
        "                    prev_date = tmp.loc[tmp[date_col] < last_date, date_col].max()\n",
        "\n",
        "                    now = tmp[tmp[date_col] == last_date].groupby(dim)[metric_col].sum()\n",
        "                    prev = tmp[tmp[date_col] == prev_date].groupby(dim)[metric_col].sum()\n",
        "\n",
        "                    movers = (now - prev).sort_values(ascending=False).dropna()\n",
        "                    st.caption(f\"Top movers: which segments changed most ({prev_date.date()} ‚Üí {last_date.date()}).\")\n",
        "\n",
        "                    movers_df = movers.head(top_n).reset_index()\n",
        "                    movers_df.columns = [dim, \"delta\"]\n",
        "                    st.dataframe(movers_df, use_container_width=True)\n",
        "\n",
        "    st.session_state[\"upledge_driver_insights\"] = driver_insights\n",
        "\n",
        "# Anomalies\n",
        "with tab4:\n",
        "    section_help(\n",
        "        \"Anomalies\",\n",
        "        \"Flags unusual spikes/drops in your metric over time. This is a signal to investigate, not a final explanation.\",\n",
        "    )\n",
        "\n",
        "    anomalies_insights: List[dict] = []\n",
        "\n",
        "    if not (caps.anomaly_ready and date_col and date_col in df.columns and df[date_col].notna().any()):\n",
        "        st.info(\"Anomaly-ready is ‚ùå ‚Äî anomaly detection is disabled.\")\n",
        "    else:\n",
        "        ts = df.groupby(date_col)[metric_col].sum().sort_index()\n",
        "        ts_df = ts.reset_index().rename(columns={date_col: \"date\", metric_col: \"value\"})\n",
        "        ts_df[\"value\"] = ts_df[\"value\"].astype(float)\n",
        "\n",
        "        scores = _detect_anomalies(ts_df[\"value\"], z_thresh=z_thresh)\n",
        "        if scores.empty:\n",
        "            st.write(\"No strong anomalies detected at current sensitivity.\")\n",
        "        else:\n",
        "            scores[\"date\"] = ts_df.loc[scores[\"idx\"], \"date\"].values\n",
        "            scores = scores[[\"date\", \"value\", \"score\", \"type\"]].sort_values(\"score\", ascending=False)\n",
        "            st.dataframe(scores, use_container_width=True)\n",
        "\n",
        "            for _, r in scores.head(5).iterrows():\n",
        "                d = r[\"date\"]\n",
        "                val = float(r[\"value\"])\n",
        "                typ = str(r[\"type\"])\n",
        "                sc = float(r[\"score\"])\n",
        "                anomalies_insights.append(\n",
        "                    _make_insight(\n",
        "                        1 if typ == \"drop\" else 2,\n",
        "                        f\"Anomalous {typ} detected on {pd.to_datetime(d).date()}\",\n",
        "                        f\"Value: **{val:,.2f}** | Robust Z: **{sc:.2f}**\",\n",
        "                        \"Investigate pipeline + business context (stockouts, missing data, one-off event).\",\n",
        "                        [\"anomaly\", typ],\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        st.line_chart(ts_df.set_index(\"date\")[[\"value\"]])\n",
        "\n",
        "    st.session_state[\"upledge_anomaly_insights\"] = anomalies_insights\n",
        "\n",
        "# LLM Narrative (placeholder)\n",
        "with tab5:\n",
        "    section_help(\n",
        "        \"LLM Narrative (optional)\",\n",
        "        \"Generates a plain-English narrative using computed summaries only. This is currently a placeholder output until you connect an API.\",\n",
        "    )\n",
        "\n",
        "    if not llm_on:\n",
        "        st.info(\"Enable the LLM narrator above to generate a narrative.\")\n",
        "    else:\n",
        "        summary_payload = {\n",
        "            \"dataset_type\": dataset_kind,\n",
        "            \"health_score\": vr.health_score,\n",
        "            \"capabilities\": asdict(vr.capabilities),\n",
        "            \"schema\": {\"time_col\": date_col, \"metric_col\": metric_col, \"dimensions\": dimensions},\n",
        "            \"top_insights\": (st.session_state.get(\"upledge_driver_insights\", []) + st.session_state.get(\"upledge_anomaly_insights\", []))[:10],\n",
        "        }\n",
        "        prompt = build_llm_prompt(summary_payload)\n",
        "        provider_short = \"openai\" if \"ChatGPT\" in llm_provider else \"gemini\"\n",
        "\n",
        "        narrative = call_llm_placeholder(provider_short, llm_key, prompt) if llm_key else call_llm_placeholder(provider_short, \"\", prompt)\n",
        "\n",
        "        st.text_area(\"Narrative output\", value=narrative, height=360)\n",
        "        with st.expander(\"Prompt sent to LLM (debug)\"):\n",
        "            st.code(prompt)\n",
        "\n",
        "# Export\n",
        "st.divider()\n",
        "section_help(\n",
        "    \"Export\",\n",
        "    \"Download the ranked insights table (CSV) or a simple Markdown report for sharing with stakeholders.\",\n",
        ")\n",
        "\n",
        "actionable = []\n",
        "actionable.extend(st.session_state.get(\"upledge_driver_insights\", []))\n",
        "actionable.extend(st.session_state.get(\"upledge_anomaly_insights\", []))\n",
        "\n",
        "if actionable:\n",
        "    out = pd.DataFrame(actionable).sort_values([\"Priority\"]).reset_index(drop=True)\n",
        "    st.dataframe(out, use_container_width=True)\n",
        "\n",
        "    if enable_downloads:\n",
        "        st.download_button(\n",
        "            \"Download insights (CSV)\",\n",
        "            data=out.to_csv(index=False).encode(\"utf-8\"),\n",
        "            file_name=\"upledge_insights.csv\",\n",
        "            mime=\"text/csv\",\n",
        "        )\n",
        "\n",
        "        md_lines = []\n",
        "        md_lines.append(\"# uPledge ‚Äî Data Summary & Insights\\n\")\n",
        "        md_lines.append(f\"- Dataset type: **{dataset_kind}**\\n\")\n",
        "        md_lines.append(f\"- Health score: **{vr.health_score}/100**\\n\")\n",
        "        md_lines.append(f\"- Metric: **{metric_col}**\\n\")\n",
        "        md_lines.append(f\"- Time column: **{date_col if date_col else '(none)'}**\\n\")\n",
        "        md_lines.append(f\"- Dimensions: **{', '.join(dimensions) if dimensions else '(none)'}**\\n\")\n",
        "        md_lines.append(\"\\n## Capability Badges\\n\")\n",
        "        md_lines.append(f\"- Trend-ready: {'YES' if caps.trend_ready else 'NO'}\\n\")\n",
        "        md_lines.append(f\"- Driver-ready: {'YES' if caps.driver_ready else 'NO'}\\n\")\n",
        "        md_lines.append(f\"- Segment-ready: {'YES' if caps.segment_ready else 'NO'}\\n\")\n",
        "        md_lines.append(f\"- Anomaly-ready: {'YES' if caps.anomaly_ready else 'NO'}\\n\")\n",
        "        md_lines.append(f\"- Investor-ready: {'YES' if caps.investor_ready else 'NO'}\\n\")\n",
        "\n",
        "        md_lines.append(\"\\n## Ranked Actionable Insights\\n\")\n",
        "        for _, row in out.iterrows():\n",
        "            md_lines.append(f\"### (P{row['Priority']}) {row['Insight']}\\n\")\n",
        "            md_lines.append(f\"- Evidence: {row['Evidence']}\\n\")\n",
        "            md_lines.append(f\"- Suggested action: {row['Suggested action']}\\n\")\n",
        "            md_lines.append(f\"- Tags: {row['Tags']}\\n\")\n",
        "\n",
        "        st.download_button(\n",
        "            \"Download report (Markdown)\",\n",
        "            data=\"\\n\".join(md_lines).encode(\"utf-8\"),\n",
        "            file_name=\"upledge_report.md\",\n",
        "            mime=\"text/markdown\",\n",
        "        )\n",
        "else:\n",
        "    st.info(\"No actionable insights generated yet. Select dimensions and/or ensure trend/anomaly readiness.\")\n",
        "\n",
        "st.caption(\"Next: replace LLM placeholder with real Gemini/OpenAI calls + add dataset-specific insight logic per option.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIHzGFRBei2G",
        "outputId": "16626671-5003-4471-d271-cf39ba1dd3a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f streamlit || true\n",
        "!streamlit run app.py --server.port 8501 --server.address 0.0.0.0 \\\n",
        "  --server.headless true --server.enableCORS false --server.enableXsrfProtection false \\\n",
        "  &>/content/streamlit.log &\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21uNNYgN6Y_E",
        "outputId": "620913a0-4797-4f10-f35a-a406bb25da27"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -I http://127.0.0.1:8501 | head\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usxM8RDI6cU3",
        "outputId": "b999200a-6226-469b-dd95-72371d00d2dc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "curl: (7) Failed to connect to 127.0.0.1 port 8501 after 0 ms: Connection refused\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_window(8501)\n",
        "print(\"Proceess done ‚úÖ\")\n",
        "print(\"Please use Chrome browser to use the web app link above\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "cAtnraY46ed_",
        "outputId": "58b9b0cc-f827-43b1-8589-43bed0764f8f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(8501, \"/\", \"https://localhost:8501/\", window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proceess done ‚úÖ\n",
            "Please use Chrome browser to use the web app link above\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"text-align:center; margin-top:40px;\">\n",
        "\n",
        "  <h2>üöÄ Want to Go Further?</h2>\n",
        "\n",
        "  <p style=\"font-size:16px; max-width:600px; margin:0 auto 24px;\">\n",
        "    This notebook is just a starting point.\n",
        "    If you‚Äôd like to explore this with your own data or in a real-world setup, we‚Äôre happy to help.\n",
        "  </p>\n",
        "\n",
        "  <a href=\"https://upledge.io/contact-us\"\n",
        "     target=\"_blank\"\n",
        "     rel=\"noopener noreferrer\"\n",
        "     style=\"\n",
        "       display:inline-block;\n",
        "       padding:14px 28px;\n",
        "       background-color:#2563EB;\n",
        "       color:#ffffff;\n",
        "       text-decoration:none;\n",
        "       border-radius:8px;\n",
        "       font-size:16px;\n",
        "       font-weight:600;\n",
        "       box-shadow:0 4px 10px rgba(0,0,0,0.15);\n",
        "     \">\n",
        "     Contact Us\n",
        "  </a>\n",
        "\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "EkD2gSCccltU"
      }
    }
  ]
}