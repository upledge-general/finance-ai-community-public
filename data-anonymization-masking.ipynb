{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/upledge-general/finance-ai-community-public/blob/main/data-anonymization-masking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFXmLgLlUWSb"
      },
      "source": [
        "# ğŸ“Œ Demo Notebook â€” Please Read First\n",
        "\n",
        "Welcome! This is the exact notebook used in the live demo.\n",
        "\n",
        "**How to use:**\n",
        "1. Go to `Runtime` â†’ `Run all`.\n",
        "2. Wait until all cells finish (youâ€™ll see âœ… messages).\n",
        "3. Scroll and explore the outputs.\n",
        "\n",
        "> ğŸ”’ **Don't worry:** This original version is read-only.\n",
        "> If you want to experiment, go to `File` â†’ `Save a copy in Drive` and play in *your* copy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ6CLhNxVQQr"
      },
      "source": [
        "# Data Anonymization / Masking / Redaction for LLM Use\n",
        "\n",
        "Prepare **real-world data safely for LLM analysis** by automatically removing, hiding, or anonymizing sensitive information.\n",
        "\n",
        "This notebook is designed for **non-technical users**.  \n",
        "You donâ€™t need to edit any code.\n",
        "\n",
        "---\n",
        "\n",
        "## How this notebook works\n",
        "\n",
        "1. Click **Runtime â†’ Run all**\n",
        "2. Upload your file **or** paste your text\n",
        "3. Choose:\n",
        "   - Processing mode (**Redact / Mask / Anonymize**)\n",
        "   - Policy level (**Light / Standard / Strict**)\n",
        "4. Click **â–¶ Generate**\n",
        "5. Download the sanitized outputs and audit files\n",
        "\n",
        "Thatâ€™s it.\n",
        "\n",
        "---\n",
        "\n",
        "## What happens behind the scenes\n",
        "\n",
        "When you run this notebook, it will automatically:\n",
        "\n",
        "- Prepare the required environment  \n",
        "- Accept your uploaded file(s) or pasted text  \n",
        "- Detect sensitive information such as:\n",
        "  - emails, phone numbers, IDs\n",
        "  - names, organisations, locations, addresses\n",
        "  - money amounts, dates, tokens (Strict mode)\n",
        "- Apply the selected transformation:\n",
        "  - **Redact** â†’ remove\n",
        "  - **Mask** â†’ partially hide\n",
        "  - **Anonymize** â†’ replace with safe placeholders\n",
        "- Generate:\n",
        "  - sanitized text / tables\n",
        "  - a redaction log\n",
        "  - a confidence report\n",
        "  - an internal mapping dictionary\n",
        "\n",
        "If something cannot be processed, the notebook will show a **clear, human-readable message** explaining why.\n",
        "\n",
        "---\n",
        "\n",
        "## First-time users (recommended)\n",
        "\n",
        "For your first run, start with:\n",
        "- **Pasted text**, or  \n",
        "- A small **sample CSV**\n",
        "\n",
        "This lets you quickly see the full anonymization flow before using real data.\n",
        "\n",
        "You can re-run the notebook as many times as you like with different settings.\n",
        "\n",
        "---\n",
        "\n",
        "## Supported input formats\n",
        "\n",
        "**Processed in this notebook**\n",
        "- Text (`.txt`, `.md`)\n",
        "- CSV (`.csv`)\n",
        "- Excel (`.xlsx`)\n",
        "- PDF *(text-based only)*\n",
        "- JSON / JSONL (`.json`, `.jsonl`)\n",
        "- Pasted raw text\n",
        "\n",
        "**Not processed (advanced tier)**\n",
        "- Scanned PDFs\n",
        "- Images (`.png`, `.jpg`, etc.)\n",
        "- Word / PowerPoint files (`.docx`, `.pptx`)\n",
        "\n",
        "For these, the notebook will prompt you to **contact us** for advanced processing (OCR / layout parsing).\n",
        "\n",
        "---\n",
        "\n",
        "## About enable NERâ€ option\n",
        "\n",
        "When enabled, the notebook will also use AI models to detect:\n",
        "- Person names\n",
        "- Organisation names\n",
        "- Locations\n",
        "- Full addresses\n",
        "\n",
        "This provides deeper protection but may take slightly longer to run.\n",
        "\n",
        "You can turn it off for faster, rule-based processing.\n",
        "\n",
        "---\n",
        "\n",
        "## Tip\n",
        "\n",
        "This notebook is safe to explore.  \n",
        "Nothing is sent to external services.\n",
        "\n",
        "If you want to try different settings, simply:\n",
        "- change the options\n",
        "- click **â–¶ Generate** again\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO1tHrAPeAzw"
      },
      "source": [
        "#### Install & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XA3ZFSfHDhs",
        "outputId": "e1fce772-6db9-45ed-f245-7bbbc5367ca5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.54.0-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Collecting cachetools<7,>=5.5 (from streamlit)\n",
            "  Downloading cachetools-6.2.6-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.46)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (26.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.6)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.3)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.26.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2026.1.4)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.54.0-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cachetools-6.2.6-py3-none-any.whl (11 kB)\n",
            "Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cachetools, pydeck, streamlit\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 7.0.0\n",
            "    Uninstalling cachetools-7.0.0:\n",
            "      Successfully uninstalled cachetools-7.0.0\n",
            "Successfully installed cachetools-6.2.6 pydeck-0.9.1 streamlit-1.54.0\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m330.6/330.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pyopenssl 24.2.1 requires cryptography<44,>=41.0.5, but you have cryptography 46.0.5 which is incompatible.\n",
            "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "âœ… Setup complete\n"
          ]
        }
      ],
      "source": [
        "# Install packages (once)\n",
        "# uncomment this if you run for the first time/comment if you want to rerun multiple time\n",
        "!pip install streamlit\n",
        "!pip install -q pypdf\n",
        "!pip install -q presidio-analyzer presidio-anonymizer spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "print(\"âœ… Setup complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9-nJGUzeFcY"
      },
      "source": [
        "#### Web App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2ugKLjkzfuId",
        "outputId": "b8685bc5-02f9-4c2e-c026-f8f21b0787c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "# app.py\n",
        "# ============================================================\n",
        "# Data Anonymization / Masking / Redaction for LLM Use\n",
        "# Phase 1 (Regex) + Phase 2 (Presidio + spaCy NER)\n",
        "#\n",
        "# Supported (MVP):\n",
        "#   .txt .md .csv .xlsx .pdf (text-based) .json .jsonl\n",
        "#\n",
        "# Advanced tier (skipped with message):\n",
        "#   images (.png .jpg .jpeg .tiff .bmp .webp), .docx, .pptx, scanned PDFs\n",
        "#\n",
        "# Phase 2 NER:\n",
        "#   - Runs only for Standard & Strict\n",
        "#   - Uses Presidio Analyzer with spaCy en_core_web_sm\n",
        "#   - Adds PERSON / ORG / LOCATION detections\n",
        "#   - Adds ADDRESS detection \"beyond basic\" via:\n",
        "#       (a) strong multi-pattern address span recognizer (regex-based, multi-lingual hints)\n",
        "#       (b) upgrade rule: LOCATION -> ADDRESS when combined with postcode/street/unit cues\n",
        "#\n",
        "# Outputs:\n",
        "#   - Anonymized text / Masked tables / Sanitized JSON(L)\n",
        "#   - Redaction log (regex vs NER vs ADDRESS detector)\n",
        "#   - Mapping dictionary (internal)\n",
        "#   - Confidence report\n",
        "#\n",
        "# Run:\n",
        "#   streamlit run app.py\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import io\n",
        "import re\n",
        "import json\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "from typing import Any, Dict, List, Tuple, Optional\n",
        "\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "\n",
        "# Optional PDF text extraction\n",
        "try:\n",
        "    from pypdf import PdfReader  # type: ignore\n",
        "    PYPDF_AVAILABLE = True\n",
        "except Exception:\n",
        "    PYPDF_AVAILABLE = False\n",
        "\n",
        "# Optional NER (Phase 2)\n",
        "try:\n",
        "    from presidio_analyzer import AnalyzerEngine  # type: ignore\n",
        "    from presidio_analyzer.nlp_engine import NlpEngineProvider  # type: ignore\n",
        "    PRESIDIO_AVAILABLE = True\n",
        "except Exception:\n",
        "    PRESIDIO_AVAILABLE = False\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Product constants\n",
        "# ---------------------------\n",
        "SUPPORTED_SIMPLE = {\"txt\", \"md\", \"csv\", \"xlsx\", \"pdf\", \"json\", \"jsonl\"}\n",
        "ADVANCED_TIER = {\"png\", \"jpg\", \"jpeg\", \"tiff\", \"bmp\", \"webp\", \"docx\", \"pptx\"}\n",
        "\n",
        "CONTACT_URL = \"https://upledge.io/contact-us\"  # replace if needed\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Step 0: Policy definitions\n",
        "# ---------------------------\n",
        "# NOTE:\n",
        "# - Standard / Strict include PERSON/ORG/LOCATION (via NER), and ADDRESS (custom detector).\n",
        "POLICIES: Dict[str, List[str]] = {\n",
        "    \"Light\": [\"EMAIL\", \"PHONE\", \"ID\"],\n",
        "    \"Standard\": [\"EMAIL\", \"PHONE\", \"ID\", \"IP\", \"URL\", \"PERSON\", \"ORG\", \"LOCATION\", \"ADDRESS\"],\n",
        "    \"Strict\": [\n",
        "        \"EMAIL\", \"PHONE\", \"ID\", \"IP\", \"URL\", \"PERSON\", \"ORG\", \"LOCATION\", \"ADDRESS\",\n",
        "        \"CREDIT_CARD\", \"MONEY\", \"DATE\", \"TOKEN\", \"BEARER\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Detection patterns (Phase 1)\n",
        "# ---------------------------\n",
        "RE_EMAIL = re.compile(r\"\\b[A-Za-z0-9._%+\\-]+@[A-Za-z0-9.\\-]+\\.[A-Za-z]{2,}\\b\")\n",
        "RE_PHONE = re.compile(\n",
        "    r\"(?<!\\d)\"\n",
        "    r\"(?:\\+?\\d{1,3}[\\s\\-]?)?\"\n",
        "    r\"(?:\\(?\\d{2,4}\\)?[\\s\\-]?)?\"\n",
        "    r\"\\d{3,4}[\\s\\-]?\\d{3,4}\"\n",
        "    r\"(?!\\d)\"\n",
        ")\n",
        "RE_IP = re.compile(r\"\\b(?:(?:25[0-5]|2[0-4]\\d|1?\\d?\\d)\\.){3}(?:25[0-5]|2[0-4]\\d|1?\\d?\\d)\\b\")\n",
        "RE_URL = re.compile(r\"\\bhttps?://[^\\s]+\\b\", re.IGNORECASE)\n",
        "RE_ID_GENERIC = re.compile(r\"\\b[A-Z]{1,3}\\d{6,12}\\b\")  # generic IDs: ABC1234567\n",
        "\n",
        "# Credit card (loose) + Luhn validation\n",
        "RE_CC = re.compile(r\"\\b(?:\\d[ -]*?){13,19}\\b\")\n",
        "\n",
        "# Strict extras\n",
        "RE_MONEY = re.compile(\n",
        "    r\"\\b(?:RM|MYR|\\$|USD|SGD|EUR|GBP)\\s?\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\b\"\n",
        "    r\"|\\b\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\s?(?:RM|MYR|USD|SGD|EUR|GBP)\\b\"\n",
        ")\n",
        "RE_DATE = re.compile(r\"\\b(?:\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}|\\d{4}[/-]\\d{1,2}[/-]\\d{1,2})\\b\")\n",
        "RE_TOKEN = re.compile(r\"\\b(?:sk-|AIza|ghp_|xoxb-|xoxa-|AKIA)[A-Za-z0-9_\\-]{10,}\\b\")\n",
        "RE_BEARER = re.compile(r\"\\bBearer\\s+[A-Za-z0-9\\-\\._~\\+\\/]+=*\\b\", re.IGNORECASE)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Address detection (Phase 2 - beyond basic)\n",
        "# ---------------------------\n",
        "# The goal: detect real-world address spans reliably enough for redaction/masking,\n",
        "# without claiming perfect postal normalization.\n",
        "#\n",
        "# Strategy:\n",
        "# 1) Detect strong address lines using multiple patterns:\n",
        "#    - unit/floor/block patterns (Unit/No/Level/Lot/Blk/Block)\n",
        "#    - street indicators (Jalan/Jln/Lorong/Persiaran/Taman/Street/Road/Ave/etc.)\n",
        "#    - postcode patterns (MY/SG/UK/US-like)\n",
        "#    - state/city hints (common MY states + generic City/Province/State words)\n",
        "# 2) Create spans and score them. Keep spans above a threshold.\n",
        "# 3) Upgrade NER LOCATION spans to ADDRESS if they overlap or are adjacent to strong cues.\n",
        "\n",
        "RE_POSTCODE_MY = re.compile(r\"\\b\\d{5}\\b\")  # Malaysia\n",
        "RE_POSTCODE_SG = re.compile(r\"\\b\\d{6}\\b\")  # Singapore\n",
        "RE_POSTCODE_UK = re.compile(r\"\\b[A-Z]{1,2}\\d[A-Z\\d]?\\s?\\d[A-Z]{2}\\b\", re.IGNORECASE)\n",
        "RE_POSTCODE_US = re.compile(r\"\\b\\d{5}(?:-\\d{4})?\\b\")\n",
        "\n",
        "RE_UNIT = re.compile(\n",
        "    r\"\\b(?:no\\.?|unit|suite|ste\\.?|apt\\.?|apartment|floor|lvl|level|lot|blok|blk|block|bldg|building)\\b\"\n",
        "    r\"(?:\\s*[-#:]*\\s*[A-Za-z0-9]+)?\",\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "RE_STREET = re.compile(\n",
        "    r\"\\b(?:jalan|jln|lorong|persiaran|taman|lebuh|kg\\.?|kampung|\"\n",
        "    r\"street|st\\.|road|rd\\.|avenue|ave\\.|lane|ln\\.|drive|dr\\.|boulevard|blvd\\.|\"\n",
        "    r\"place|pl\\.|court|ct\\.|crescent|cr\\.|highway|hwy\\.|\"\n",
        "    r\"pusat|kompleks|seksyen|section|parcel|phase)\\b\",\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "RE_LOCALITY_HINT = re.compile(\n",
        "    r\"\\b(?:city|bandar|daerah|district|state|negeri|province|wilayah|\"\n",
        "    r\"kedah|kelantan|terengganu|pahang|perlis|perak|selangor|johor|melaka|malacca|\"\n",
        "    r\"penang|pulau pinang|sabah|sarawak|\"\n",
        "    r\"kl|kuala lumpur|putrajaya|labuan)\\b\",\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "# A \"line-ish\" address candidate: length-limited chunk that includes at least a street cue,\n",
        "# plus optionally unit or postcode. We capture up to 140 chars to avoid grabbing whole paragraphs.\n",
        "RE_ADDRESS_LINE = re.compile(\n",
        "    r\"(?P<addr>\"\n",
        "    r\"(?:.{0,20}\\b(?:no\\.?|unit|lot|lvl|level|floor|blok|blk|block)\\b.{0,40})?\"\n",
        "    r\".{0,40}\\b(?:jalan|jln|lorong|persiaran|taman|street|st\\.|road|rd\\.|avenue|ave\\.|lane|ln\\.|drive|dr\\.)\\b\"\n",
        "    r\".{0,80}\"\n",
        "    r\"(?:\\b\\d{5}\\b|\\b\\d{6}\\b|\\b[A-Z]{1,2}\\d[A-Z\\d]?\\s?\\d[A-Z]{2}\\b|\\b\\d{5}(?:-\\d{4})?\\b)?\"\n",
        "    r\")\",\n",
        "    re.IGNORECASE | re.DOTALL\n",
        ")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Helpers\n",
        "# ---------------------------\n",
        "def get_ext(filename: str) -> str:\n",
        "    if not filename or \".\" not in filename:\n",
        "        return \"\"\n",
        "    return filename.rsplit(\".\", 1)[-1].lower().strip()\n",
        "\n",
        "\n",
        "def sha_token(value: str, prefix: str, salt: str) -> str:\n",
        "    h = hashlib.sha256((salt + value).encode(\"utf-8\")).hexdigest()[:8]\n",
        "    return f\"{prefix}_{h}\"\n",
        "\n",
        "\n",
        "def luhn_check(raw: str) -> bool:\n",
        "    digits = [c for c in raw if c.isdigit()]\n",
        "    if len(digits) < 13 or len(digits) > 19:\n",
        "        return False\n",
        "    s = 0\n",
        "    parity = len(digits) % 2\n",
        "    for i, ch in enumerate(digits):\n",
        "        d = int(ch)\n",
        "        if i % 2 == parity:\n",
        "            d *= 2\n",
        "            if d > 9:\n",
        "                d -= 9\n",
        "        s += d\n",
        "    return s % 10 == 0\n",
        "\n",
        "\n",
        "def mask_email(email: str) -> str:\n",
        "    parts = email.split(\"@\", 1)\n",
        "    if len(parts) != 2:\n",
        "        return \"[MASKED:EMAIL]\"\n",
        "    user, dom = parts\n",
        "    if len(user) <= 1:\n",
        "        user_m = \"*\"\n",
        "    elif len(user) == 2:\n",
        "        user_m = user[0] + \"*\"\n",
        "    else:\n",
        "        user_m = user[0] + \"***\" + user[-1]\n",
        "    return f\"{user_m}@{dom}\"\n",
        "\n",
        "\n",
        "def mask_phone(phone: str) -> str:\n",
        "    digits = \"\".join([c for c in phone if c.isdigit()])\n",
        "    if len(digits) <= 4:\n",
        "        return \"[MASKED:PHONE]\"\n",
        "    return \"****\" + digits[-4:]\n",
        "\n",
        "\n",
        "def advanced_tier_message(ext: str) -> str:\n",
        "    return (\n",
        "        f\"This input (`.{ext}`) is in a **higher complexity tier**.\\n\\n\"\n",
        "        \"It typically requires **advanced processing** such as OCR (for scanned docs/images) \"\n",
        "        \"or layout-aware parsing (for DOCX/PPTX).\\n\\n\"\n",
        "        f\"If you'd like to enable this, please contact us: {CONTACT_URL}\"\n",
        "    )\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Span utilities (safe replacement)\n",
        "# ---------------------------\n",
        "def drop_overlapping_spans(spans: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Keep non-overlapping spans. Prefer:\n",
        "      1) earlier start\n",
        "      2) longer length\n",
        "      3) higher score (if present)\n",
        "    \"\"\"\n",
        "    def sort_key(s: Dict[str, Any]):\n",
        "        length = (s[\"end\"] - s[\"start\"])\n",
        "        score = float(s.get(\"score\", 0.0) or 0.0)\n",
        "        return (s[\"start\"], -length, -score)\n",
        "\n",
        "    spans_sorted = sorted(spans, key=sort_key)\n",
        "    kept: List[Dict[str, Any]] = []\n",
        "    last_end = -1\n",
        "    for s in spans_sorted:\n",
        "        if s[\"start\"] >= last_end:\n",
        "            kept.append(s)\n",
        "            last_end = s[\"end\"]\n",
        "    return kept\n",
        "\n",
        "\n",
        "def apply_span_replacements(text: str, spans: List[Dict[str, Any]]) -> str:\n",
        "    \"\"\"\n",
        "    Apply replacements from right-to-left so offsets remain valid.\n",
        "    spans must have: start, end, replacement\n",
        "    \"\"\"\n",
        "    out = text\n",
        "    for s in sorted(spans, key=lambda x: (x[\"start\"], x[\"end\"]), reverse=True):\n",
        "        out = out[:s[\"start\"]] + s[\"replacement\"] + out[s[\"end\"]:]\n",
        "    return out\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# PDF extraction (text-based only)\n",
        "# ---------------------------\n",
        "def extract_pdf_text(pdf_bytes: bytes) -> Tuple[str, Dict[str, Any]]:\n",
        "    if not PYPDF_AVAILABLE:\n",
        "        return \"\", {\"error\": \"pypdf is not available. Install pypdf to enable PDF extraction.\"}\n",
        "\n",
        "    reader = PdfReader(io.BytesIO(pdf_bytes))\n",
        "    pages = []\n",
        "    empty_pages = 0\n",
        "\n",
        "    for i, page in enumerate(reader.pages):\n",
        "        try:\n",
        "            t = page.extract_text() or \"\"\n",
        "        except Exception:\n",
        "            t = \"\"\n",
        "        if not t.strip():\n",
        "            empty_pages += 1\n",
        "        pages.append(f\"--- Page {i+1} ---\\n{t}\")\n",
        "\n",
        "    text = \"\\n\\n\".join(pages).strip()\n",
        "    likely_scanned = (len(reader.pages) > 0) and ((empty_pages / max(1, len(reader.pages))) > 0.6)\n",
        "\n",
        "    meta = {\"pages\": len(reader.pages), \"empty_pages\": empty_pages, \"likely_scanned\": likely_scanned}\n",
        "    return text, meta\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Presidio Analyzer (cached)\n",
        "# ---------------------------\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def get_presidio_analyzer() -> Optional[\"AnalyzerEngine\"]:\n",
        "    if not PRESIDIO_AVAILABLE:\n",
        "        return None\n",
        "    provider = NlpEngineProvider(\n",
        "        nlp_configuration={\n",
        "            \"nlp_engine_name\": \"spacy\",\n",
        "            \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_sm\"}],\n",
        "        }\n",
        "    )\n",
        "    nlp_engine = provider.create_engine()\n",
        "    return AnalyzerEngine(nlp_engine=nlp_engine, supported_languages=[\"en\"])\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Address detector (custom spans)\n",
        "# ---------------------------\n",
        "def detect_address_spans(text: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Returns list of span dicts: start/end/original/entity_type/score/detector.\n",
        "    Scoring is heuristic but stronger than \"basic\":\n",
        "      - Street cue present -> +0.45\n",
        "      - Unit cue present -> +0.20\n",
        "      - Postcode present -> +0.25\n",
        "      - Locality/state/city hint -> +0.10\n",
        "      - Starts with number (common in addresses) -> +0.05\n",
        "    Keep spans with score >= 0.55\n",
        "    \"\"\"\n",
        "    spans: List[Dict[str, Any]] = []\n",
        "\n",
        "    for m in RE_ADDRESS_LINE.finditer(text):\n",
        "        addr = (m.group(\"addr\") or \"\").strip()\n",
        "        if not addr:\n",
        "            continue\n",
        "\n",
        "        start = m.start(\"addr\")\n",
        "        end = m.end(\"addr\")\n",
        "        snippet = addr\n",
        "\n",
        "        score = 0.0\n",
        "        if RE_STREET.search(snippet):\n",
        "            score += 0.45\n",
        "        if RE_UNIT.search(snippet):\n",
        "            score += 0.20\n",
        "        if RE_POSTCODE_MY.search(snippet) or RE_POSTCODE_SG.search(snippet) or RE_POSTCODE_UK.search(snippet) or RE_POSTCODE_US.search(snippet):\n",
        "            score += 0.25\n",
        "        if RE_LOCALITY_HINT.search(snippet):\n",
        "            score += 0.10\n",
        "        if re.match(r\"^\\s*\\d{1,5}\\b\", snippet):\n",
        "            score += 0.05\n",
        "\n",
        "        # Avoid absurdly long captures (fallback safety)\n",
        "        if (end - start) > 180:\n",
        "            continue\n",
        "\n",
        "        if score >= 0.55:\n",
        "            spans.append({\n",
        "                \"entity_type\": \"ADDRESS\",\n",
        "                \"start\": start,\n",
        "                \"end\": end,\n",
        "                \"original\": text[start:end],\n",
        "                \"score\": round(score, 3),\n",
        "                \"detector\": \"ADDRESS_REGEX\",\n",
        "            })\n",
        "\n",
        "    return spans\n",
        "\n",
        "\n",
        "def upgrade_location_to_address(\n",
        "    text: str,\n",
        "    ner_spans: List[Dict[str, Any]]\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    If a LOCATION span overlaps OR is close to strong address cues,\n",
        "    upgrade it to ADDRESS.\n",
        "    \"\"\"\n",
        "    upgraded = []\n",
        "    for s in ner_spans:\n",
        "        if s[\"entity_type\"] != \"LOCATION\":\n",
        "            upgraded.append(s)\n",
        "            continue\n",
        "\n",
        "        # Examine a small window around the location\n",
        "        w_start = max(0, s[\"start\"] - 30)\n",
        "        w_end = min(len(text), s[\"end\"] + 50)\n",
        "        window = text[w_start:w_end]\n",
        "\n",
        "        evidence = 0\n",
        "        if RE_STREET.search(window):\n",
        "            evidence += 1\n",
        "        if RE_UNIT.search(window):\n",
        "            evidence += 1\n",
        "        if RE_POSTCODE_MY.search(window) or RE_POSTCODE_SG.search(window) or RE_POSTCODE_UK.search(window) or RE_POSTCODE_US.search(window):\n",
        "            evidence += 1\n",
        "        if RE_LOCALITY_HINT.search(window):\n",
        "            evidence += 1\n",
        "\n",
        "        if evidence >= 2:\n",
        "            s2 = dict(s)\n",
        "            s2[\"entity_type\"] = \"ADDRESS\"\n",
        "            s2[\"detector\"] = \"NER_UPGRADED_TO_ADDRESS\"\n",
        "            # nudge score a bit for reporting\n",
        "            s2[\"score\"] = float(s2.get(\"score\", 0.6) or 0.6) + 0.1\n",
        "            upgraded.append(s2)\n",
        "        else:\n",
        "            upgraded.append(s)\n",
        "\n",
        "    return upgraded\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Core: replace decision\n",
        "# ---------------------------\n",
        "def compute_replacement(\n",
        "    raw: str,\n",
        "    entity_type: str,\n",
        "    mode: str,\n",
        "    salt: str,\n",
        "    mapping: Dict[str, str]\n",
        ") -> str:\n",
        "    if mode == \"Redact\":\n",
        "        return f\"[REDACTED:{entity_type}]\"\n",
        "\n",
        "    if mode == \"Mask\":\n",
        "        if entity_type == \"EMAIL\":\n",
        "            return mask_email(raw)\n",
        "        if entity_type == \"PHONE\":\n",
        "            return mask_phone(raw)\n",
        "        if entity_type == \"CREDIT_CARD\":\n",
        "            digits = \"\".join([c for c in raw if c.isdigit()])\n",
        "            return \"****\" + (digits[-4:] if len(digits) >= 4 else \"\")\n",
        "        # Generic mask for other entity types\n",
        "        return f\"[MASKED:{entity_type}]\"\n",
        "\n",
        "    # Anonymize\n",
        "    key = f\"{entity_type}::{raw}\"\n",
        "    if key not in mapping:\n",
        "        mapping[key] = sha_token(raw, entity_type, salt)\n",
        "    return mapping[key]\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Core: transform text (Phase 1 + Phase 2)\n",
        "# ---------------------------\n",
        "def transform_text(\n",
        "    text: str,\n",
        "    mode: str,          # \"Redact\" | \"Mask\" | \"Anonymize\"\n",
        "    policy_name: str,   # \"Light\" | \"Standard\" | \"Strict\"\n",
        "    salt: str,\n",
        "    source: str,\n",
        "    enable_ner: bool,\n",
        ") -> Tuple[str, List[Dict[str, Any]], Dict[str, str], Dict[str, Any]]:\n",
        "    enabled = set(POLICIES.get(policy_name, []))\n",
        "    mapping: Dict[str, str] = {}\n",
        "    log: List[Dict[str, Any]] = []\n",
        "\n",
        "    # ---- Phase 1: Regex detections as span list (not re.sub), so we can merge with NER safely.\n",
        "    # For regex, we build spans from the ORIGINAL text first.\n",
        "    regex_detectors: List[Tuple[str, re.Pattern]] = []\n",
        "    if \"EMAIL\" in enabled:\n",
        "        regex_detectors.append((\"EMAIL\", RE_EMAIL))\n",
        "    if \"URL\" in enabled:\n",
        "        regex_detectors.append((\"URL\", RE_URL))\n",
        "    if \"IP\" in enabled:\n",
        "        regex_detectors.append((\"IP\", RE_IP))\n",
        "    if \"PHONE\" in enabled:\n",
        "        regex_detectors.append((\"PHONE\", RE_PHONE))\n",
        "    if \"CREDIT_CARD\" in enabled:\n",
        "        regex_detectors.append((\"CREDIT_CARD\", RE_CC))\n",
        "    if \"ID\" in enabled:\n",
        "        regex_detectors.append((\"ID\", RE_ID_GENERIC))\n",
        "    if \"TOKEN\" in enabled:\n",
        "        regex_detectors.append((\"TOKEN\", RE_TOKEN))\n",
        "    if \"BEARER\" in enabled:\n",
        "        regex_detectors.append((\"BEARER\", RE_BEARER))\n",
        "    if \"MONEY\" in enabled:\n",
        "        regex_detectors.append((\"MONEY\", RE_MONEY))\n",
        "    if \"DATE\" in enabled:\n",
        "        regex_detectors.append((\"DATE\", RE_DATE))\n",
        "\n",
        "    spans: List[Dict[str, Any]] = []\n",
        "\n",
        "    for etype, pattern in regex_detectors:\n",
        "        for m in pattern.finditer(text):\n",
        "            raw = m.group(0)\n",
        "\n",
        "            # Reduce false positives for credit cards\n",
        "            if etype == \"CREDIT_CARD\" and not luhn_check(raw):\n",
        "                continue\n",
        "\n",
        "            spans.append({\n",
        "                \"source\": source,\n",
        "                \"entity_type\": etype,\n",
        "                \"original\": raw,\n",
        "                \"start\": m.start(),\n",
        "                \"end\": m.end(),\n",
        "                \"score\": 0.99,  # regex detections are \"high confidence\" for what they match\n",
        "                \"detector\": \"REGEX\",\n",
        "            })\n",
        "\n",
        "    # ---- Phase 2: ADDRESS detector (custom, beyond basic), only for Standard/Strict and only if enabled.\n",
        "    if enable_ner and policy_name in {\"Standard\", \"Strict\"} and \"ADDRESS\" in enabled:\n",
        "        addr_spans = detect_address_spans(text)\n",
        "        # attach metadata\n",
        "        for s in addr_spans:\n",
        "            s.update({\"source\": source})\n",
        "        spans.extend(addr_spans)\n",
        "\n",
        "    # ---- Phase 2: NER (Presidio), only for Standard/Strict and only if enabled.\n",
        "    if enable_ner and policy_name in {\"Standard\", \"Strict\"} and PRESIDIO_AVAILABLE:\n",
        "        analyzer = get_presidio_analyzer()\n",
        "        if analyzer is not None:\n",
        "            entities = []\n",
        "            if \"PERSON\" in enabled:\n",
        "                entities.append(\"PERSON\")\n",
        "            if \"ORG\" in enabled:\n",
        "                entities.append(\"ORGANIZATION\")\n",
        "            if \"LOCATION\" in enabled:\n",
        "                entities.append(\"LOCATION\")\n",
        "\n",
        "            ner_results = analyzer.analyze(\n",
        "                text=text,\n",
        "                language=\"en\",\n",
        "                entities=entities if entities else None,\n",
        "            )\n",
        "\n",
        "            ner_spans: List[Dict[str, Any]] = []\n",
        "            for r in ner_results:\n",
        "                etype = r.entity_type\n",
        "                if etype == \"ORGANIZATION\":\n",
        "                    etype = \"ORG\"\n",
        "                raw = text[r.start:r.end]\n",
        "\n",
        "                ner_spans.append({\n",
        "                    \"source\": source,\n",
        "                    \"entity_type\": etype,\n",
        "                    \"original\": raw,\n",
        "                    \"start\": r.start,\n",
        "                    \"end\": r.end,\n",
        "                    \"score\": float(getattr(r, \"score\", 0.6) or 0.6),\n",
        "                    \"detector\": \"NER\",\n",
        "                })\n",
        "\n",
        "            # Upgrade LOCATION -> ADDRESS when strong evidence exists around span\n",
        "            if \"ADDRESS\" in enabled:\n",
        "                ner_spans = upgrade_location_to_address(text, ner_spans)\n",
        "\n",
        "            spans.extend(ner_spans)\n",
        "\n",
        "    # If NER requested but Presidio missing, we still run Phase 1 + address regex. We'll report this later.\n",
        "\n",
        "    # ---- Merge spans: remove overlaps\n",
        "    spans = drop_overlapping_spans(spans)\n",
        "\n",
        "    # ---- Compute replacements\n",
        "    for s in spans:\n",
        "        s[\"replacement\"] = compute_replacement(\n",
        "            raw=s[\"original\"],\n",
        "            entity_type=s[\"entity_type\"],\n",
        "            mode=mode,\n",
        "            salt=salt,\n",
        "            mapping=mapping,\n",
        "        )\n",
        "        log.append({\n",
        "            \"source\": s[\"source\"],\n",
        "            \"entity_type\": s[\"entity_type\"],\n",
        "            \"original\": s[\"original\"],\n",
        "            \"replacement\": s[\"replacement\"],\n",
        "            \"start\": s[\"start\"],\n",
        "            \"end\": s[\"end\"],\n",
        "            \"score\": s.get(\"score\", None),\n",
        "            \"detector\": s.get(\"detector\", None),\n",
        "        })\n",
        "\n",
        "    # ---- Apply replacements\n",
        "    transformed = apply_span_replacements(text, spans)\n",
        "\n",
        "    # ---- Confidence report\n",
        "    counts: Dict[str, int] = {}\n",
        "    counts_by_detector: Dict[str, int] = {}\n",
        "    for row in log:\n",
        "        counts[row[\"entity_type\"]] = counts.get(row[\"entity_type\"], 0) + 1\n",
        "        det = row.get(\"detector\") or \"UNKNOWN\"\n",
        "        counts_by_detector[det] = counts_by_detector.get(det, 0) + 1\n",
        "\n",
        "    limitations = [\n",
        "        \"Regex detection is deterministic but can miss contextual PII.\",\n",
        "    ]\n",
        "    if policy_name in {\"Standard\", \"Strict\"}:\n",
        "        if enable_ner:\n",
        "            if not PRESIDIO_AVAILABLE:\n",
        "                limitations.append(\"NER was enabled, but Presidio/spaCy is not installed in this environment.\")\n",
        "            else:\n",
        "                limitations.append(\"NER (Presidio + spaCy) is enabled for PERSON/ORG/LOCATION; accuracy varies by text quality.\")\n",
        "        else:\n",
        "            limitations.append(\"NER is disabled; Standard/Strict will not remove person/org names unless caught by regex.\")\n",
        "        limitations.append(\"ADDRESS detection uses heuristic multi-pattern matching; it may over/under-redact in edge cases.\")\n",
        "\n",
        "    confidence = {\n",
        "        \"source\": source,\n",
        "        \"policy\": policy_name,\n",
        "        \"mode\": mode,\n",
        "        \"enable_ner\": enable_ner,\n",
        "        \"total_replacements\": len(log),\n",
        "        \"counts_by_entity\": counts,\n",
        "        \"counts_by_detector\": counts_by_detector,\n",
        "        \"limitations\": limitations,\n",
        "        \"risk_flags\": [\n",
        "            \"Review outputs for false positives/negatives when handling legal/compliance-grade documents.\",\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    mapping_out = {f\"{k}\": v for k, v in mapping.items()}\n",
        "    return transformed, log, mapping_out, confidence\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Tables: process dataframe\n",
        "# ---------------------------\n",
        "def process_dataframe(\n",
        "    df: pd.DataFrame,\n",
        "    mode: str,\n",
        "    policy: str,\n",
        "    salt: str,\n",
        "    source: str,\n",
        "    enable_ner: bool,\n",
        ") -> Tuple[pd.DataFrame, List[Dict[str, Any]], Dict[str, str], Dict[str, Any]]:\n",
        "    df_out = df.copy()\n",
        "    logs_all: List[Dict[str, Any]] = []\n",
        "    mapping_all: Dict[str, str] = {}\n",
        "    counts_by_entity: Dict[str, int] = {}\n",
        "    counts_by_detector: Dict[str, int] = {}\n",
        "\n",
        "    for col in df_out.columns:\n",
        "        # Keep numeric columns as-is (avoid breaking analytics)\n",
        "        if pd.api.types.is_numeric_dtype(df_out[col]):\n",
        "            continue\n",
        "\n",
        "        new_vals = []\n",
        "        for idx, val in enumerate(df_out[col].astype(str).tolist()):\n",
        "            t, log, mapping, _ = transform_text(\n",
        "                text=val,\n",
        "                mode=mode,\n",
        "                policy_name=policy,\n",
        "                salt=salt,\n",
        "                source=source,\n",
        "                enable_ner=enable_ner,\n",
        "            )\n",
        "            new_vals.append(t)\n",
        "\n",
        "            for r in log:\n",
        "                r.update({\"column\": str(col), \"row_index\": idx})\n",
        "                logs_all.append(r)\n",
        "                counts_by_entity[r[\"entity_type\"]] = counts_by_entity.get(r[\"entity_type\"], 0) + 1\n",
        "                det = r.get(\"detector\") or \"UNKNOWN\"\n",
        "                counts_by_detector[det] = counts_by_detector.get(det, 0) + 1\n",
        "\n",
        "            mapping_all.update({f\"{source}::{k}\": v for k, v in mapping.items()})\n",
        "\n",
        "        df_out[col] = new_vals\n",
        "\n",
        "    confidence = {\n",
        "        \"source\": source,\n",
        "        \"policy\": policy,\n",
        "        \"mode\": mode,\n",
        "        \"enable_ner\": enable_ner,\n",
        "        \"total_replacements\": sum(counts_by_entity.values()),\n",
        "        \"counts_by_entity\": counts_by_entity,\n",
        "        \"counts_by_detector\": counts_by_detector,\n",
        "        \"limitations\": [\n",
        "            \"String-like columns are processed cell-by-cell; numeric columns are left unchanged.\",\n",
        "            \"If IDs or account numbers are stored as numeric types, they may not be altered.\",\n",
        "        ],\n",
        "    }\n",
        "    return df_out, logs_all, mapping_all, confidence\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# JSON/JSONL: sanitize string values recursively\n",
        "# ---------------------------\n",
        "def sanitize_json_obj(\n",
        "    obj: Any,\n",
        "    mode: str,\n",
        "    policy: str,\n",
        "    salt: str,\n",
        "    source: str,\n",
        "    enable_ner: bool,\n",
        "    path: str = \"$\",\n",
        ") -> Tuple[Any, List[Dict[str, Any]], Dict[str, str], Dict[str, Any]]:\n",
        "    logs_all: List[Dict[str, Any]] = []\n",
        "    mapping_all: Dict[str, str] = {}\n",
        "\n",
        "    if isinstance(obj, str):\n",
        "        t, log, mapping, conf = transform_text(\n",
        "            text=obj,\n",
        "            mode=mode,\n",
        "            policy_name=policy,\n",
        "            salt=salt,\n",
        "            source=source,\n",
        "            enable_ner=enable_ner,\n",
        "        )\n",
        "        for r in log:\n",
        "            r.update({\"json_path\": path})\n",
        "        logs_all.extend(log)\n",
        "        mapping_all.update({f\"{source}::{k}\": v for k, v in mapping.items()})\n",
        "        return t, logs_all, mapping_all, conf\n",
        "\n",
        "    if isinstance(obj, list):\n",
        "        out_list = []\n",
        "        conf_any = None\n",
        "        for i, item in enumerate(obj):\n",
        "            v, l, m, c = sanitize_json_obj(item, mode, policy, salt, source, enable_ner, path=f\"{path}[{i}]\")\n",
        "            out_list.append(v)\n",
        "            logs_all.extend(l)\n",
        "            mapping_all.update(m)\n",
        "            conf_any = conf_any or c\n",
        "        return out_list, logs_all, mapping_all, conf_any or {}\n",
        "\n",
        "    if isinstance(obj, dict):\n",
        "        out_dict = {}\n",
        "        conf_any = None\n",
        "        for k, v in obj.items():\n",
        "            vv, l, m, c = sanitize_json_obj(v, mode, policy, salt, source, enable_ner, path=f\"{path}.{k}\")\n",
        "            out_dict[k] = vv\n",
        "            logs_all.extend(l)\n",
        "            mapping_all.update(m)\n",
        "            conf_any = conf_any or c\n",
        "        return out_dict, logs_all, mapping_all, conf_any or {}\n",
        "\n",
        "    return obj, logs_all, mapping_all, {}\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Streamlit UI\n",
        "# ---------------------------\n",
        "st.set_page_config(page_title=\"LLM Data Anonymization (Regex mode) + NER mode)\", layout=\"wide\")\n",
        "st.title(\"Data Anonymization / Masking / Redaction for LLM Use\")\n",
        "\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "### Please select input and policy mode to begin\n",
        "**Choose input**\n",
        "- Upload file(s) **OR** paste raw text\n",
        "\n",
        "**Choose processing mode**\n",
        "- **Redact**: remove sensitive data\n",
        "- **Mask**: partially hide sensitive data\n",
        "- **Anonymize**: replace with placeholders (best for LLM reasoning)\n",
        "\n",
        "**Choose policy level**\n",
        "- **Light**: email / phone / IDs (Regex mode)\n",
        "- **Standard**: Light + PERSON/ORG/LOCATION + ADDRESS (NER mode enable)\n",
        "- **Strict**: Standard + amounts + dates + internal IDs + tokens (NER mode enable)\n",
        "\n",
        "**Supported in this app**\n",
        "- `.txt .md .csv .xlsx .pdf (text-based) .json .jsonl`\n",
        "\n",
        "**Advanced tier (not processed)**\n",
        "- images / scanned PDFs / `.docx` / `.pptx` â†’ requires OCR or layout-aware parsing\n",
        "If uploaded, the app will show a message to contact us.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\"Settings\")\n",
        "    mode = st.selectbox(\"Mode\", [\"Anonymize\", \"Mask\", \"Redact\"])\n",
        "    policy = st.selectbox(\"Policy Level\", [\"Light\", \"Standard\", \"Strict\"])\n",
        "    salt = st.text_input(\"Session Salt (stable anonymized tokens)\", value=\"upledge-demo-salt\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    enable_ner = st.checkbox(\"Enable this button (PERSON/ORG/LOCATION + ADDRESS) if you want â€œaggressiveâ€ masking\", value=True)\n",
        "\n",
        "    if enable_ner and policy in {\"Standard\", \"Strict\"} and not PRESIDIO_AVAILABLE:\n",
        "        st.warning(\"NER is enabled but Presidio/spaCy isn't available. Install presidio-analyzer + spacy + en_core_web_sm.\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    if not PYPDF_AVAILABLE:\n",
        "        st.warning(\"PDF extraction is disabled (pypdf not available). Install `pypdf` to enable text-based PDF support.\")\n",
        "\n",
        "    st.link_button(\"Contact us (OCR / advanced tier)\", CONTACT_URL)\n",
        "\n",
        "left, right = st.columns([1, 1])\n",
        "\n",
        "with left:\n",
        "    st.subheader(\"Option A: Upload file(s)\")\n",
        "    uploaded = st.file_uploader(\n",
        "        \"Upload (multi-file supported)\",\n",
        "        type=sorted(list(SUPPORTED_SIMPLE | ADVANCED_TIER)),\n",
        "        accept_multiple_files=True,\n",
        "    )\n",
        "\n",
        "with right:\n",
        "    st.subheader(\"Option B: Paste raw text\")\n",
        "    pasted = st.text_area(\n",
        "        \"Paste text here (optional)\",\n",
        "        height=240,\n",
        "        placeholder=\"Paste transcript, email, notes, logs, etc.\",\n",
        "    )\n",
        "\n",
        "run = st.button(\"Generate\", type=\"primary\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Execute run\n",
        "# ---------------------------\n",
        "if run:\n",
        "    started_at = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    st.success(f\"Run started at {started_at}\")\n",
        "\n",
        "    outputs: List[Tuple[str, Any]] = []\n",
        "    logs_all: List[Dict[str, Any]] = []\n",
        "    mapping_all: Dict[str, str] = {}\n",
        "    confidence_reports: List[Dict[str, Any]] = []\n",
        "    skipped: List[Dict[str, str]] = []\n",
        "\n",
        "    # 1) Pasted text\n",
        "    if pasted and pasted.strip():\n",
        "        out_text, log, mapping, conf = transform_text(\n",
        "            text=pasted.strip(),\n",
        "            mode=mode,\n",
        "            policy_name=policy,\n",
        "            salt=salt,\n",
        "            source=\"pasted_text\",\n",
        "            enable_ner=enable_ner,\n",
        "        )\n",
        "        outputs.append((\"pasted_text.sanitized.txt\", out_text))\n",
        "        logs_all.extend(log)\n",
        "        mapping_all.update({f\"pasted_text::{k}\": v for k, v in mapping.items()})\n",
        "        confidence_reports.append(conf)\n",
        "\n",
        "    # 2) Uploaded files\n",
        "    if uploaded:\n",
        "        for f in uploaded:\n",
        "            filename = f.name\n",
        "            ext = get_ext(filename)\n",
        "            raw = f.read()\n",
        "\n",
        "            # Advanced tier gate\n",
        "            if ext in ADVANCED_TIER:\n",
        "                skipped.append({\"file\": filename, \"reason\": f\"Advanced tier (.{ext})\"})\n",
        "                st.warning(f\"âš ï¸ Skipped: {filename}\")\n",
        "                st.info(advanced_tier_message(ext))\n",
        "                continue\n",
        "\n",
        "            # Unsupported gate\n",
        "            if ext not in SUPPORTED_SIMPLE:\n",
        "                skipped.append({\"file\": filename, \"reason\": f\"Unsupported type (.{ext or 'unknown'})\"})\n",
        "                st.warning(f\"âš ï¸ Skipped: {filename}\")\n",
        "                st.info(advanced_tier_message(ext or \"unknown\"))\n",
        "                continue\n",
        "\n",
        "            # Supported handlers\n",
        "            if ext in {\"txt\", \"md\"}:\n",
        "                text = raw.decode(\"utf-8\", errors=\"replace\")\n",
        "                out_text, log, mapping, conf = transform_text(text, mode, policy, salt, source=filename, enable_ner=enable_ner)\n",
        "                outputs.append((f\"{filename}.sanitized.txt\", out_text))\n",
        "                logs_all.extend(log)\n",
        "                mapping_all.update({f\"{filename}::{k}\": v for k, v in mapping.items()})\n",
        "                confidence_reports.append(conf)\n",
        "\n",
        "            elif ext == \"pdf\":\n",
        "                text, meta = extract_pdf_text(raw)\n",
        "\n",
        "                if not text.strip() or meta.get(\"likely_scanned\"):\n",
        "                    skipped.append({\"file\": filename, \"reason\": \"Likely scanned PDF (OCR required)\"})\n",
        "                    st.warning(f\"âš ï¸ Skipped: {filename}\")\n",
        "                    st.info(advanced_tier_message(\"scanned pdf\"))\n",
        "                    continue\n",
        "\n",
        "                out_text, log, mapping, conf = transform_text(text, mode, policy, salt, source=filename, enable_ner=enable_ner)\n",
        "                conf = {**conf, \"pdf_meta\": meta}\n",
        "                outputs.append((f\"{filename}.sanitized.txt\", out_text))\n",
        "                logs_all.extend(log)\n",
        "                mapping_all.update({f\"{filename}::{k}\": v for k, v in mapping.items()})\n",
        "                confidence_reports.append(conf)\n",
        "\n",
        "            elif ext == \"csv\":\n",
        "                df = pd.read_csv(io.BytesIO(raw))\n",
        "                df_out, log, mapping, conf = process_dataframe(df, mode, policy, salt, source=filename, enable_ner=enable_ner)\n",
        "                outputs.append((f\"{filename}.sanitized.csv\", df_out))\n",
        "                logs_all.extend(log)\n",
        "                mapping_all.update(mapping)\n",
        "                confidence_reports.append(conf)\n",
        "\n",
        "            elif ext == \"xlsx\":\n",
        "                df = pd.read_excel(io.BytesIO(raw))\n",
        "                df_out, log, mapping, conf = process_dataframe(df, mode, policy, salt, source=filename, enable_ner=enable_ner)\n",
        "                outputs.append((f\"{filename}.sanitized.xlsx\", df_out))\n",
        "                logs_all.extend(log)\n",
        "                mapping_all.update(mapping)\n",
        "                confidence_reports.append(conf)\n",
        "\n",
        "            elif ext == \"json\":\n",
        "                data = json.loads(raw.decode(\"utf-8\", errors=\"replace\"))\n",
        "                sanitized, log, mapping, conf = sanitize_json_obj(data, mode, policy, salt, source=filename, enable_ner=enable_ner)\n",
        "                outputs.append((f\"{filename}.sanitized.json\", json.dumps(sanitized, ensure_ascii=False, indent=2)))\n",
        "                logs_all.extend(log)\n",
        "                mapping_all.update(mapping)\n",
        "                confidence_reports.append(conf or {\"source\": filename})\n",
        "\n",
        "            elif ext == \"jsonl\":\n",
        "                lines = raw.decode(\"utf-8\", errors=\"replace\").splitlines()\n",
        "                out_lines: List[str] = []\n",
        "                log_acc: List[Dict[str, Any]] = []\n",
        "                map_acc: Dict[str, str] = {}\n",
        "                conf_any = None\n",
        "\n",
        "                for i, line in enumerate(lines):\n",
        "                    if not line.strip():\n",
        "                        continue\n",
        "                    obj = json.loads(line)\n",
        "                    sanitized, log, mapping, conf = sanitize_json_obj(\n",
        "                        obj, mode, policy, salt, source=filename, enable_ner=enable_ner, path=f\"$[line={i}]\"\n",
        "                    )\n",
        "                    out_lines.append(json.dumps(sanitized, ensure_ascii=False))\n",
        "                    log_acc.extend(log)\n",
        "                    map_acc.update(mapping)\n",
        "                    conf_any = conf_any or conf\n",
        "\n",
        "                outputs.append((f\"{filename}.sanitized.jsonl\", \"\\n\".join(out_lines)))\n",
        "                logs_all.extend(log_acc)\n",
        "                mapping_all.update(map_acc)\n",
        "                confidence_reports.append(conf_any or {\"source\": filename})\n",
        "\n",
        "    # ---------------------------\n",
        "    # Display outputs\n",
        "    # ---------------------------\n",
        "    st.markdown(\"---\")\n",
        "    st.subheader(\"Outputs\")\n",
        "\n",
        "    if not outputs:\n",
        "        st.warning(\"No supported inputs were processed. Upload a supported file or paste text.\")\n",
        "    else:\n",
        "        for fname, payload in outputs:\n",
        "            st.write(f\"**{fname}**\")\n",
        "            if isinstance(payload, pd.DataFrame):\n",
        "                st.dataframe(payload, use_container_width=True)\n",
        "                csv_bytes = payload.to_csv(index=False).encode(\"utf-8\")\n",
        "                st.download_button(\n",
        "                    \"Download table as CSV\",\n",
        "                    data=csv_bytes,\n",
        "                    file_name=fname.replace(\".xlsx\", \".csv\"),\n",
        "                    mime=\"text/csv\",\n",
        "                    key=f\"dl_{fname}\",\n",
        "                )\n",
        "            else:\n",
        "                preview = str(payload)\n",
        "                st.text_area(\"Preview\", value=preview[:20000], height=220, key=f\"prev_{fname}\")\n",
        "                st.download_button(\n",
        "                    \"Download file\",\n",
        "                    data=preview.encode(\"utf-8\"),\n",
        "                    file_name=fname,\n",
        "                    mime=\"text/plain\",\n",
        "                    key=f\"dl_{fname}\",\n",
        "                )\n",
        "            st.markdown(\"---\")\n",
        "\n",
        "    # ---------------------------\n",
        "    # Audit artifacts\n",
        "    # ---------------------------\n",
        "    st.subheader(\"Audit Artifacts\")\n",
        "\n",
        "    if skipped:\n",
        "        st.write(\"**Skipped inputs**\")\n",
        "        st.dataframe(pd.DataFrame(skipped), use_container_width=True)\n",
        "\n",
        "    if logs_all:\n",
        "        df_log = pd.DataFrame(logs_all)\n",
        "        st.write(\"**Redaction / Masking Log**\")\n",
        "        st.dataframe(df_log.head(800), use_container_width=True)\n",
        "        st.download_button(\n",
        "            \"Download redaction_log.csv\",\n",
        "            data=df_log.to_csv(index=False).encode(\"utf-8\"),\n",
        "            file_name=\"redaction_log.csv\",\n",
        "            mime=\"text/csv\",\n",
        "        )\n",
        "    else:\n",
        "        st.info(\"No log entries (either nothing detected or no supported inputs were processed).\")\n",
        "\n",
        "    # Confidence report\n",
        "    conf_payload = {\n",
        "        \"generated_at\": started_at,\n",
        "        \"global_settings\": {\"mode\": mode, \"policy\": policy, \"enable_ner\": enable_ner},\n",
        "        \"reports\": confidence_reports,\n",
        "    }\n",
        "    st.write(\"**Confidence Report**\")\n",
        "    st.json(conf_payload)\n",
        "    st.download_button(\n",
        "        \"Download confidence_report.json\",\n",
        "        data=json.dumps(conf_payload, ensure_ascii=False, indent=2).encode(\"utf-8\"),\n",
        "        file_name=\"confidence_report.json\",\n",
        "        mime=\"application/json\",\n",
        "    )\n",
        "\n",
        "    # Mapping dictionary (internal)\n",
        "    st.write(\"**Mapping Dictionary (internal)**\")\n",
        "    st.caption(\"Keep this internal. Sharing it externally can re-identify the data.\")\n",
        "    preview_map = dict(list(mapping_all.items())[:120])\n",
        "    st.json(preview_map)\n",
        "    st.download_button(\n",
        "        \"Download mapping_dictionary.json\",\n",
        "        data=json.dumps(mapping_all, ensure_ascii=False, indent=2).encode(\"utf-8\"),\n",
        "        file_name=\"mapping_dictionary.json\",\n",
        "        mime=\"application/json\",\n",
        "    )\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.caption(\"Standard/Strict Mode: Enable Presidio+spaCy NER (PERSON/ORG/LOCATION) + stronger ADDRESS detection. Advanced tier still requires OCR/layout parsing. Please contact us for advanced use case\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r4z893UFxibn",
        "outputId": "77434390-ccd5-4d7b-ae49-d62eabe5798f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pkill -f streamlit || true\n",
        "!streamlit run app.py --server.port 8501 --server.address 0.0.0.0 \\\n",
        "  --server.headless true --server.enableCORS false --server.enableXsrfProtection false \\\n",
        "  &>/content/streamlit.log &\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rG3oiTGjxzQ9",
        "outputId": "afb139bc-4d63-431e-9eb0-19da7f3adbf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "curl: (7) Failed to connect to 127.0.0.1 port 8501 after 0 ms: Connection refused\n"
          ]
        }
      ],
      "source": [
        "!curl -I http://127.0.0.1:8501 | head\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WjHmSAj-x2mm",
        "outputId": "972b8783-2470-4bff-9f55-8ae6a2a75564"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mWarning: This function may stop working due to changes in browser security.\n",
            "Try `serve_kernel_port_as_iframe` instead. \u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "(async (port, path, text, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port);\n",
              "    const anchor = document.createElement('a');\n",
              "    anchor.href = new URL(path, url).toString();\n",
              "    anchor.target = '_blank';\n",
              "    anchor.setAttribute('data-href', url + path);\n",
              "    anchor.textContent = text;\n",
              "    element.appendChild(anchor);\n",
              "  })(8501, \"/\", \"https://localhost:8501/\", window.element)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Proceess done âœ…\n",
            "Please use Chrome browser to use the web app link above\n"
          ]
        }
      ],
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_window(8501)\n",
        "print(\"Proceess done âœ…\")\n",
        "print(\"Please use Chrome browser to use the web app link above\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDAonMzsbu-_"
      },
      "source": [
        "<div style=\"text-align:center; margin-top:40px;\">\n",
        "\n",
        "  <h2>ğŸš€ Want to Go Further?</h2>\n",
        "\n",
        "  <p style=\"font-size:16px; max-width:600px; margin:0 auto 24px;\">\n",
        "    This notebook is just a starting point.\n",
        "    If youâ€™d like to explore this with your own data or in a real-world setup, weâ€™re happy to help.\n",
        "  </p>\n",
        "\n",
        "  <a href=\"https://upledge.io/contact-us\"\n",
        "     target=\"_blank\"\n",
        "     rel=\"noopener noreferrer\"\n",
        "     style=\"\n",
        "       display:inline-block;\n",
        "       padding:14px 28px;\n",
        "       background-color:#2563EB;\n",
        "       color:#ffffff;\n",
        "       text-decoration:none;\n",
        "       border-radius:8px;\n",
        "       font-size:16px;\n",
        "       font-weight:600;\n",
        "       box-shadow:0 4px 10px rgba(0,0,0,0.15);\n",
        "     \">\n",
        "     Contact Us\n",
        "  </a>\n",
        "\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOX5RXEt4kczqeQip/Rgi3b",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}